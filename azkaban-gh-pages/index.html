<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<title>Azkaban 3.0 中文文档</title>

				<link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
    <!-- Bootstrap core CSS -->
    <link href="./css/bootstrap.css" rel="stylesheet">
		<link href="./css/azkaban.css" rel="stylesheet">
		<link href="./css/lightbox.css" rel="stylesheet">

		<script type="text/javascript" src="./js/jquery-1.9.1.js"></script>
		<script type="text/javascript" src="./js/bootstrap.min.js"></script>
		<script type="text/javascript" src="./js/lightbox-2.6.min.js"></script>


	</head>
	<body data-spy="scroll" data-target=".bs-sidebar">
		    <script type="text/javascript">
      function navMenuClick(url) {
        window.location.href = url;
      }
    </script>
    <div class="navbar navbar-inverse navbar-static-top">
      <div class="container">
				<div class="navbar-header">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
					<div class="navbar-logo">
						<a href="/">Azkaban</a>
					</div>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li onClick="navMenuClick('http://azkaban.github.io/downloads.html')"><a href="http://azkaban.github.io/downloads.html">Downloads</a></li>
						<li class="active" onClick="navMenuClick('/')"><a href="/">Documentation</a></li>
            <li onClick="navMenuClick('http://azkaban.github.io//contributing.html')"><a href="http://azkaban.github.io/contributing.html">Contributing</a></li>
            <li onClick="navMenuClick('http://azkaban.github.io/license.html')"><a href="http://azkaban.github.io/license.html">License</a></li>
            <li onClick="navMenuClick('http://azkaban.github.io/contact.html')"><a href="http://azkaban.github.io/contact.html">Contact</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown">GitHub <b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><a href="https://github.com/azkaban/azkaban">azkaban repo</a></li>
                <li><a href="https://github.com/azkaban/azkaban/issues">azkaban issues</a></li>
                <li class="divider"></li>
                <li><a href="https://github.com/azkaban/azkaban-plugins">azkaban-plugins repo</a></li>
                <li><a href="https://github.com/azkaban/azkaban-plugins/issues">azkaban-plugins issues</a></li>
              </ul>
            </li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>



		<div class="az-page-header">
			<div class="container">
				<div class="row">
					<div class="col-lg-12">
						<h1>Azkaban 3.0 中文文档</h1>
					</div>
				</div>
			</div>
		</div>

		<div class="container">
			<div class="row">
			
				
				<div class="col-md-3">
					<div class="bs-sidebar hidden-print" role="complementary">
     <ul class="bs-sidenav nav">
        <li>
            <a href="#overview">概述</a>
         </li>
		<li>
			<a href="#getting-started">使用入门</a>
			<ul class="nav">
	          <li><a href="#building-from-source">从源代码构建</a></li>
	          <li><a href="#solo-setup">Solo服务器安装</a></li>
			  <li><a href="#database-setup">数据库安装</a></li>
			  <li><a href="#webserver-setup">Web服务器安装</a></li>
			  <li><a href="#executor-setup">Executor安装</a></li>
			  <li><a href="#plugin-setup">Plugin安装</a></li>
			  <li><a href="#multipleExecutors-config">为多Executor模式配置执行程序</a></li>
			  <li><a href="#property-override">属性覆盖</a></li>
			  <li><a href="#upgrade-21">从2.1升级</a></li>
			  <li><a href="#upgrade-27">从2.7升级</a></li>
			</ul>
		</li>
		<li>
			<a href="#configuration">配置</a>
			<ul class="nav">
				<li><a href="#azkaban-webserver">AzkabanWebServer配置</a></li>
				<li><a href="#azkaban-execserver">AzkabanExecutorServer配置</a></li>
				<li><a href="#azkaban-plugin-configuration">插件配置</a></li>
			</ul>
		</li>
		<li>
			<a href="#user-manager">用户管理器</a>
			<ul class="nav">
				<li><a href="#xml-usermanager">XmlUserManager</a></li>
				<li><a href="#custom-usermanager">自定义UserManager</a></li>
			</ul>
		</li>
		<li>
			<a href="#creating-flows">创建流程</a>
			<ul class="nav">
				<li><a href="#job-configuration">作业配置</a></li>
				<li><a href="#builtin-jobtypes">内置JobTypes</a></li>
			</ul>
		</li>
		<li>
			<a href="#using-azkaban">使用Azkaban</a>
			<ul class="nav">
				<li><a href="#create-projects">创建项目</a></li>
				<li><a href="#upload-projects">上传项目</a></li>
				<li><a href="#flow-view">流程视图</a></li>
				<li><a href="#project-permissions">项目权限</a></li>
				<li><a href="#executing-flows">执行流程</a></li>
				<li><a href="#executions">执行</a></li>
				<li><a href="#schedule-flow">调度计划流程</a></li>
				<li><a href="#job-page">作业页面</a></li>
				<li><a href="#job-details">作业详情</a></li>				
				<li><a href="#system-stats">系统统计</a></li>
			</ul>
		</li>
		<li>
	      <a href="#ajax-api">AJAX接口</a>
	      <ul class="nav">
	          <li><a href="#ajax-api">概述</a></li>
	          <li><a href="#api-authenticate">验证</a></li>
	          <li><a href="#api-create-a-project">创建项目</a></li>
	          <li><a href="#api-delete-a-project">删除项目</a></li>
	          <li><a href="#api-upload-a-project-zip">上传项目Zip包</a></li>
	          <li><a href="#api-fetch-flows-of-a-project">获取项目的流程</a></li>
	          <li><a href="#api-fetch-jobs-of-a-flow">获取流程的作业</a></li>
	          <li><a href="#api-fetch-executions-of-a-flow">获取流程的执行</a></li>
	          <li><a href="#api-fetch-running-executions-of-a-flow">获取正运行的执行</a></li>
	          <li><a href="#api-execute-a-flow">执行流程</a></li>
	          <li><a href="#api-cancel-a-flow-execution">取消流程执行</a></li>
	          <li><a href="#api-schedule-a-flow">调度期限流程（已废弃）</a></li>
	          <li><a href="#api-flexible-schedule">使用Cron灵活调度</a></li>
	          <li><a href="#api-fetch-schedule">获取计划</a></li>
	          <li><a href="#api-unschedule-a-flow">取消调度流程</a></li>
	          <li><a href="#api-set-sla">设置SLA</a></li>
	          <li><a href="#api-fetch-sla">获取SLA</a></li>
	          <li><a href="#api-pause-a-flow-execution">暂停流程执行</a></li>
	          <li><a href="#api-resume-a-flow-execution">恢复流程执行</a></li>
	          <li><a href="#api-fetch-a-flow-execution">获取流程执行</a></li>
	          <li><a href="#api-fetch-execution-job-logs">获取执行作业日志</a></li>
	          <li><a href="#api-fetch-flow-execution-updates">获取流程执行更新</a></li>
	      </ul>
	    </li>
		<li><a href="#how-to">如何做</a></li>
		<li>
			<a href="#plugins">插件</a>
			<ul class="nav">
				<li><a href="#hadoopsecuritymanager">HadoopSecurityManager</a></li>
				<li><a href="#hdfs-browser">HDFS浏览器</a></li>
				<li><a href="#jobtype-plugins">JobType插件</a></li>
	         </ul>
	     </li>
		  <li>
		    <a href="#job-types">作业类型</a>
		    <ul class="nav">
		      <li><a href="#command-type">命令Command</a></li>
		      <li><a href="#hadoopshell-type">HadoopShell</a></li>
		      <li><a href="#java-type">Java</a></li>
		      <li><a href="#hadoopjava-type">Hadoop Java</a></li>
		      <li><a href="#pig-type">Pig</a></li>
		      <li><a href="#hive-type">Hive</a></li>
		      <li><a href="#new-hive-type">Hive v2</a></li>
		      <li><a href="#voldemortbuildandpush-type">VoldemortBuildAndPush</a></li>
		      <li><a href="#common-configurations">常用配置</a></li>
		      <li><a href="#create-job-types">常见JobTypes</a></li>
		      <li><a href="#reload-jobtypes">重新加载Jobtypes</a></li>                  
		    </ul>
		  </li>
	</ul>
</div>


				</div>
				<div class="col-md-9">
					          <div class="docs-section">
            <div class="page-header">
  <h1 id="overview">概述</h1>
</div>

<p>Azkaban由LinkedIn开发实现，用以解决Hadoop作业依赖问题。 从ETL工作到数据分析产品，都需要按顺序运行作业作业。</p>

<p>最初是单一服务器解决方案，随着多年来Hadoop用户数量的增加，Azkaban已经发展成为更鲁棒的解决方案。</p>

<p>Azkaban由三个关键组成部分组成：</p>

<ul>
  <li>关系数据库（MySQL）</li>
  <li>AzkabanWebServer</li>
  <li>AzkabanExecutorServer</li>
</ul>

<img title="Azkaban Overview" src="./images/azkaban2overviewdesign.png" alt="Azkaban Overview" width="500" />

<h3>关系数据库（MySQL）</h3>

<p>Azkaban使用MySQL存储它的大部分状态，AzkabanWebServer和AzkabanExecutorServer都会访问数据库。</p>

<h4>AzkabanWebServer如何使用数据库？</h4>

<p>Web服务器使用数据库的原因如下：</p>

<ul>
  <li><strong>项目管理</strong> - 项目管理，项目权限控制，上传的文件等。</li>
  <li><strong>执行流程状态</strong> - 跟踪执行流程，跟踪执行流程的执行者。</li>
  <li><strong>上一个流程/作业</strong> - 搜索以前执行的作业和流程，以及访问其日志文件。</li>
  <li><strong>调度程序</strong> - 保存预定作业的状态</li>
  <li><strong>SLA</strong> - 保存所有的SLA规则</li>
</ul>

<h4>AzkabanExecutorServer如何使用数据库？</h4>

<p>执行者服务器使用数据库的原因如下：</p>

<ul>
  <li><strong>访问项目</strong> - 从数据库中检索项目文件。</li>
  <li><strong>执行流程/作业</strong> - 检索并更新正在执行的流程的数据。</li>
  <li><strong>日志</strong> - 将作业和流程的输出日志存储到数据库中。</li>
  <li><strong>互流依赖性</strong> - 如果流在另一个执行器上运行，它可从数据库中获取状态。</li>
</ul>

<p>没有理由不选择MySQL，它是一个广泛使用的数据库。我们也正在实现与其他数据库的兼容性，尽管对历史运行作业的搜索要求从关系数据存储更方便。</p>

<h3>AzkabanWebServer</h3>

<p>AzkabanWebServer是所有Azkaban的主要管理者。它处理项目管理，认证，调度程序和执行监控。 它也可以作为网络用户界面。</p>

<p>使用Azkaban很容易。Azkaban使用<code>*.job</code>键值属性文件来定义工作流程中的各个作业，并使用_dependencies_属性来定义作业的依赖关系链。这些作业文件和相关代码可以存档到<code>*.zip</code>并通过网络服务器通过Azkaban UI或curl上传。</p>

<h3>AzkabanExecutorServer</h3>

<p>先前版本的Azkaban在单个服务器中同时具有AzkabanWebServer和AzkabanExecutorServer功能。目前，执行者已经被分离到它自己的服务器中。分离这些服务有几个原因：我们很快就可以扩展执行次数，并在执行失败时重新运行Executors。此外，我们能够对Azkaban进行升级，而对用户的影响最小。随着Azkaban的使用增长，我们发现升级Azkaban变得越来越困难，因为一天中的所有时间都变成“高峰”。</p>


          </div>
          <div class="docs-section">
            <div class="page-header">
  <h1 id="getting-started">使用入门</h1>
</div>

<p class="lead">在3.0版本中，我们提供了三种模式：独立的“独立服务器”模式，较重的两服务器模式和分布式多执行器模式。以下描述这几种模式之间的区别。</p>

<p>在<strong>独立服务器模式</strong>下，数据库是嵌入式的H2数据库，并且Web服务器和执行器服务器都在相同的进程中运行。如果只想尝试一下，这应该很有用。它也可以用于小规模场景。</p>
<ol>
	<li><a href="#solo-setup">下载并安装独立服务器包</a></li>
	<li><a href="#plugin-setup">安装Azkaban插件</a></li>
</ol>
<p>对于<strong>两服务器模式</strong>适用于更严肃的生产环境。其数据库应该由主从设置的MySQL实例提供支持。Web服务器和执行器服务器应运行在不同的进程中，以便升级和维护不应影响用户。</p>
<ol>
	<li><a href="#database-setup">设置数据库</a></li>
	<li><a href="#webserver-setup">下载并安装Web服务器</a></li>
	<li><a href="#executor-setup">下载并安装Executor Server</a></li>
	<li><a href="#plugin-setup">安装Azkaban插件</a></li>
</ol>
<p>对于<strong>多执行器模式</strong>适用于更严肃的生产环境。其数据库应该由主从设置的MySQL实例提供支持。Web服务器和执行器服务器应运行在不同的进程中，以便升级和维护不应影响用户。这种多主机设置为Azkaban带来了强大且可扩展的方面。</p>
<ol>
	<li><a href="#database-setup">设置数据库</a></li>
	<li><a href="#webserver-setup">下载并安装Web服务器</a></li>
	<li><a href="#multipleExecutors-config">配置数据库以使用多个执行程序</a></li>
	<li><a href="#executor-setup">为数据库中配置的每个执行程序下载并安装Executor Server</a></li>
	<li><a href="#plugin-setup">安装Azkaban插件</a></li>
</ol>
<p>以下是关于如何设置Azkaban启动的说明。</p>

            <hr>
            <h2 id="building-from-source">从源代码构建</h2>

<p>Azkaban构建使用<a href="https://gradle.org/" target="_blank">Gradle</a>（使用<code>gradlew</code>即Gradle包装器）可运行时自动下载），并且需要Java 8或更高版本。</p>

<p>以下命令在Linux，OS X等*nix平台上运行。</p>
<pre>
  # Build Azkaban
  ./gradlew build

  # Clean the build
  ./gradlew clean

  # Build and install distributions
  ./gradlew installDist

  # Run tests
  ./gradlew test

  # Build without running tests
  ./gradlew build -x test
</pre>

<p>这些都是标准的Gradle命令。请查看<a href="https://gradle.org/documentation/" target="_blank">Gradle文档</a>以获取更多信息。</p>

Gradle在项目目录内创建<code>.tar.gz</code>文件。例如<code>./azkaban-solo-server/build/distributions/azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz</code>。使用<code>tar -xvzf path/to/azkaban-*.tar.gz</code>解压缩。

            <hr>
            <h2 id="solo-setup">Solo服务器入门</h2>

<p>独立服务器是Azkaban的独立实例，也是最简单的开始。独立服务器具有以下优点</p>
<ul>
  <li><strong>易于安装</strong> - 不需要MySQL实例。它使用H2作为其主要的持久性存储。</li>
  <li><strong>易于启动</strong> - Web服务器和执行器服务器都在相同的过程中运行</li>
  <li><strong>全功能</strong> - 它包装所有的Azkaban功能。您可以正常使用它并为其安装插件</li>
</ul>

<h3>安装Solo服务器</h3>
按照以下步骤开始。
<ol>
  <li><strong>克隆仓库：</strong>运行<code>git clone https://github.com/azkaban/azkaban.git</code></li>
  <li><strong>构建Azkaban并创建安装</strong>运行<code>cd azkaban; ./gradlew build installDist</code></li>
  <li><strong>启动服务器：</strong>运行<code>cd azkaban-solo-server/build/install/azkaban-solo-server; bin/azkaban-solo-start.sh</code></li>
  <li><strong>停止服务器：</strong>运行<code>bin/azkaban-solo-shutdown.sh</code>从azkaban-solo-server安装目录中</li>
</ol>

<p>另请参阅<a href="#building-from-source">从源代码构建</a> 。</p>

<p>独立服务器安装应包含以下目录。</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <th>文件夹</th>
    <th>描述</th>
  </thead>
  <tbody>
    <tr>
      <td>bin</td>
      <td>启动/停止Azkaban的Jetty服务器的脚本</td>
    </tr>
    <tr>
      <td>conf</td>
      <td>Azkaban独立服务器的配置文件</td>
    </tr>
    <tr>
      <td>lib</td>
      <td>Azkaban的jar依赖关系</td>
    </tr>
    <tr>
      <td>extlib</td>
      <td>添加到extlib的其他jar将被添加到Azkaban的类路径中</td>
    </tr>
    <tr>
      <td>plugins</td>
      <td>可以安装插件的目录</td>
    </tr>
    <tr>
      <td>web</td>
      <td>Azkaban web服务器的网页（css，javascript，image）文件</td>
    </tr>
  </tbody>
</table>

<p>在<code>conf</code>在conf目录中，应该有三个文件：</p>

<ul>
  <li><code>azkaban.private.properties</code> - 由Azkaban用于运行时参数</li>
  <li><code>azkaban.properties</code> - 由Azkaban用于运行时参数</li>
  <li><code>global.properties</code> - 作为共享属性传递给每个工作流和作业的全局静态属性。</li>
  <li><code>azkaban-users.xml</code> - 用于添加用户和角色以进行身份​​验证。 如果XmLUserManager未设置为使用此文件，则不使用此文件。</li>
</ul>

<p>其中<code>azkaban.properties</code>文件将成为主配置文件。</p>

<h3>获取SSL的KeyStore（可选）</h3>

<p>Azkaban独立服务器默认情况下不使用SSL。但是，您可以在独立的Web服务器中以相同的方式进行设置。具体如下：</p>

<p>Azkaban的Web服务器可以使用SSL套接字连接器，这意味着密钥库必须可用。您可以按照此链接提供的步骤进行操作([http://docs.codehaus.org/display/JETTY/How+to+configure+SSL](http://docs.codehaus.org/display/JETTY/How+to+configure+SSL))来创建一个。 一旦创建了密钥库文件，Azkaban必须被赋予它的位置和密码。在_azkaban.properties_中，以下属性应该被覆盖。</p>

<pre class="code">
jetty.keystore=keystore
jetty.password=password
jetty.keypassword=password
jetty.truststore=keystore
jetty.trustpassword=password
</pre>


<h3>设置UserManager</h3>

<p>Azkaban使用UserManager提供身份验证和用户角色。默认情况下，Azkaban包含并使用<code>XmlUserManager</code>，它可从_azkaban-users.xml_中获取用户名/密码和角色，如<code>azkaban.properties</code>文件中所示。</p>

<ul>
  <li><code>user.manager.class=azkaban.user.XmlUserManager</code></li>
  <li><code>user.manager.xml.file=conf/azkaban-users.xml</code></li>
</ul>

<h3>运行Web服务器</h3>

<p>在<code>azkaban.properties</code>中的以下属性用于配置jetty。</p>

<pre class="code">
jetty.maxThreads=25
jetty.ssl.port=8081
</pre>

<p>执行<code>bin/azkaban-solo-start.sh</code>启动独立服务器。要关闭，请运行<code>bin/azkaban-solo-shutdown.sh</code></p>

<p>在浏览器中打开<a href="http://localhost:8081/index">http://localhost:8081/index</a>链接</p>

            <hr>
            <h2 id="database-setup">数据库设置</h2>

<p class="lead">目前，Azkaban2仅使用MySQL作为其数据存储，尽管我们正在评估其他可能的存储系统。</p>

<h3>1. 安装MySQL</h3>
<p>这些说明不包含MySQL DB的安装，但您可以访问<a href="http://dev.mysql.com/doc/index.html" target="_blank">MySQL文档站点</a>上的说明。</p>

<h3>2. 设置数据库</h3>
<p>为Azkaban<a href="http://dev.mysql.com/doc/refman/5.7/en/create-database.html" target="_blank">创建一个数据库</a>。例如：</p>

<pre class="code">
# Example database creation command, although the db name doesn't need to be 'azkaban'
mysql&gt; CREATE DATABASE azkaban;
</pre>

<p>为Azkaban<a href="http://dev.mysql.com/doc/refman/5.7/en/create-user.html" target="_blank">创建一个数据库用户</a>。例如：</p>

<pre class="code">
# Example database creation command. The user name doesn't need to be 'azkaban'
mysql&gt; CREATE USER 'username'@'%' IDENTIFIED BY 'password';
</pre>

<p>设置数据库的<a href="http://dev.mysql.com/doc/refman/5.7/en/grant.html" target="_blank">用户权限</a>。 为Azkaban创建一个用户（如果尚未创建），并为Azkaban数据库中的所有表赋予用户<code>INSERT</code>，<code>SELECT</code>，<code>UPDATE</code>，<code>DELETE</code>权限。</p>

<pre class="code">
# Replace db, username with the ones created by the previous steps.
mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE ON &lt;database&gt;.* to '&lt;username&gt;'@'%' WITH GRANT OPTION;
</pre>

<p>配置数据包大小可能需要配置。默认情况下，MySQL可能有一个可接受的低数据包大小。为了增加它，你需要将属性<code>max_allowed_packet</code>设置为更高的值，比如1024M。</p>
<p>要在linux中配置它，请打开<code>/etc/my.cnf</code> 。在<code>mysqld</code>之后的某处，添加以下内容：</p>

<pre class="code">
[mysqld]
...
max_allowed_packet=1024M
</pre>

要重新启动MySQL，您可以运行...

<pre class="code">
$ sudo /sbin/service mysqld restart
</pre>

<h3>3. 创建Azkaban表</h3>
<p>从<a href='http://azkaban.github.io/downloads.html'>下载页面</a>下载azkaban-sql-script tarball。表创建脚本包含在此存档中。</p>
<p>在MySQL实例上运行单个表创建脚本来创建表。或者，只需运行<code>create-all-sql</code>脚本。以<code>update</code>为前缀的任何脚本都可以忽略。</p>

<h3>4. 获取JDBC连接器Jar包</h3>
<p>出于各种原因，Azkaban不会分发MySQL JDBC连接器jar。你可以从<a href="http://www.mysql.com/downloads/connector/j/" target="_blank">这个链接</a>下载jar包。</p>
<p>Web服务器和执行程序服务器都需要此jar，并应将其放入两个服务器的/extlib目录中。</p>

            <hr>
            <h2 id="webserver-setup">设置Azkaban Web服务器</h2>

<p class="lead">Azkaban Web服务器处理项目管理，认证，调度和执行触发。</p>

<h3>安装Web服务器</h3>

<p>从<a href="http://azkaban.github.io/downloads.html">下载页面</a>抓取azkaban-web-server软件包</p>

<p>或者，您可以克隆<a href="https://github.com/azkaban/azkaban2" target="_blank">GitHub仓库</a>，您可以从主分支构建最新版本。有关从源代码构建的说明，请<a href="#building-from-source">参阅此处</a></p>

<p>将包解压缩到一个目录中。安装路径应该与AzkabanExecutorServer不同。提取后，应该有以下目录：</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <th>文件夹</th>
    <th>说明</th>
  </thead>
  <tbody>
    <tr>
      <td>bin</td>
      <td>启动Azkaban的Jetty服务器的脚本</td>
    </tr>
    <tr>
      <td>conf</td>
      <td>Azkaban独立服务器的配置</td>
    </tr>
    <tr>
      <td>lib</td>
      <td>Azkaban的jar依赖关系</td>
    </tr>
    <tr>
      <td>extlib</td>
      <td>添加到extlib的其他jar将被添加到Azkaban的类路径中</td>
    </tr>
    <tr>
      <td>plugins</td>
      <td>可以安装插件的目录</td>
    </tr>
    <tr>
      <td>web</td>
      <td>Azkaban Web服务器的网页（css，javascript，image）文件</td>
    </tr>
  </tbody>
</table>

<p>在<code>conf</code>目录中，应该有三个文件</p>

<ul>
	<li><code>azkaban.properties</code> - 由Azkaban用于运行时参数</li>
	<li><code>global.properties</code> - 作为共享属性传递给每个工作流和作业的全局静态属性。
	<li><code>azkaban-users.xml</code> -用于添加用户和角色以进行身份​​验证。 如果<code>XmLUserManager</code>未设置为使用此文件，则不使用此文件。
</ul>

<p>其中<code>azkaban.properties</code>文件将是设置Azkaban所必需的主配置文件。</p>

<h3>获取SSL的KeyStore</h3>

<p>Azkaban使用SSL套接字连接器，这意味着密钥库必须可用。 您可以按照<a href="http://docs.codehaus.org/display/JETTY/How+to+configure+SSL" target="_blank">此链接</a>提供的步骤创建一个。</p>
<p>一旦创建了密钥库文件，Azkaban必须被赋予它的位置和密码。在<code>azkaban.properties</code>中下面的属性应该被覆盖。</p>

<pre class="code">
jetty.keystore=keystore
jetty.password=password
jetty.keypassword=password
jetty.truststore=keystore
jetty.trustpassword=password
</pre>

<h3>设置数据库</h3>

<p>如果你还没有得到MySQL的JDBC驱动程序，你可以从<a href="http://www.mysql.com/downloads/connector/j/" target="_blank">这个链接</a>中获得它。</p>
<p>将此jar放入<code>extlib</code>目录。所有的外部依赖应该被添加到extlib目录中。</p>

<p>要将Azkaban Web客户端指向MySQL实例，您需要将连接参数添加到<code>azkaban.properties</code>。</p>

<pre class="code">
database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100
</pre>

<p>目前MySQL是Azkaban中唯一支持的数据存储类型。所以<code>database.type</code>应该总是<code>mysql</code>。</p>

<h3>设置UserManager</h3>

<p>Azkaban使用<code>UserManager</code>提供身份验证和用户角色。/p>
<p>默认情况下，Azkaban包含并使用<code>XmlUserManager</code>，它可以从<code>azkaban.properties</code>文件中看到从<code>azkaban-users.xml</code>获取用户名/密码和角色。</p>

<pre class="code">
user.manager.class=azkaban.user.XmlUserManager
user.manager.xml.file=conf/azkaban-users.xml
</pre>

<h3>运行Web服务器</h3>

<p>在<code>azkaban.properties</code>中的以下属性用于配置jetty。</p>

<pre class="code">
jetty.maxThreads=25
jetty.ssl.port=8443
</pre>

<p>执行<code>bin/azkaban-web-start.sh</code>启动AzkabanWebServer。</p>
<p>要关闭AzkabanWebServer，请运行<code>bin/azkaban-web-shutdown.sh</code>。</p>

<p>您可以通过浏览器访问Web服务器来测试访问权限。</p>

            <hr>
            <h2 id="executor-setup">设置Azkaban执行程序服务器</h2>

<p class="lead">Azkaban执行器服务器处理工作流和作业的实际执行。</p>

<h3>安装Executor服务器</h3>

<p>从<a href="http://azkaban.github.io/downloads.html">下载页面</a>抓取azkaban-exec-server软件包。</p>

<p>或者，您可以克隆<a href="https://github.com/azkaban/azkaban2" target="_blank"GitHub仓库</a>，您可以从主分支构建最新版本。有关从源代码构建的说明，<a href="#building-from-source">请参阅此处</a>。</p>

<p>将包解压缩到一个目录中。安装路径应该与Azkaban Web服务器不同。提取后，应该有以下目录。</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <th>文件夹</th>
    <th>描述</th>
  </thead>
  <tbody>
    <tr>
      <td>bin</td>
      <td>启动Azkaban的Jetty服务器的脚本</td>
    </tr>
    <tr>
      <td>conf</td>
      <td>Azkaban独立服务器的配置</td>
    </tr>
    <tr>
      <td>lib</td>
      <td>Azkaban的jar依赖关系</td>
    </tr>
    <tr>
      <td>extlib</td>
      <td>添加到extlib的其他jar将被添加到Azkaban的类路径中</td>
    </tr>
    <tr>
      <td>plugins</td>
      <td>可以安装插件的目录</td>
    </tr>
  </tbody>
</table>

<p>在<code>conf</code>目录中，我们只需要配置<code>azkaban.properties</code>文件。</p>
<p>该文件是设置Azkaban执行程序所必需的主要配置文件。</p>

<h3>设置数据库</h3>

<p>如果你还没有得到MySQL的JDBC驱动程序，你可以从<a href="http://www.mysql.com/downloads/connector/j/" target="_blank">这个链接</a>中获得它。</p>
<p>将此jar放入<code>extlib</code>目录。所有的外部依赖应该被添加到extlib目录中。</p>

<p>要将Azkaban Web客户端指向MySQL实例，您需要将连接参数添加到<code>azkaban.properties</code>。</p>

<pre class="code">
database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100
</pre>

<p>目前MySQL是Azkaban中唯一支持的数据存储类型。所以<code>database.type</code>应该总是<code>mysql</code>。</p>

<h3>配置AzabanWebServer和AzkabanExecutorServer客户端</h3>

<p>执行服务器需要设置一个端口，AzabanWebServer需要知道这个端口是什么。</p>
<p>需要在AzkabanExecutorServer的<code>azkaban.properties</code>上设置以下属性。</p>

<pre class="code">
# Azkaban Executor settings
executor.maxThreads=50
executor.port=12321
executor.flow.threads=30
</pre>

<h4>单一执行器模式</h4>
<p>默认情况下，<code>executor.port</code>设置为<code>12321</code>。 AzkabanWebServer也必须指向这个端口。</p>
<p>这是通过在AzkabanWebServer的<code>azkaban.properties</code>设置以下属性完成的。</p>
<pre class="code">
executor.port=12321
</pre>

<h4>多执行器模式</h4>
<p></p>如果我们想要在多执行器模式下运行，我们需要在web服务器配置中启用多执行器模式。请确认您在azkaban.properties中有以下属性。azkaban.use.multiple.executors和azkaban.executorselector.comparator.*是必需的属性。请注意，单<code>azkaban.use.multiple.executors</code>不符合多执行器模式。</p>

<pre class="code">
azkaban.use.multiple.executors=true
azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus
azkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1
azkaban.executorselector.comparator.Memory=1
azkaban.executorselector.comparator.LastDispatched=1
azkaban.executorselector.comparator.CpuUsage=1
</pre>

<p>这些更改只有在重新启动服务器后才会启动。</p>

<h3>运行Executor服务器</h3>

<p>执行<code>bin/azkaban-exec-start.sh</code>启动AzkabanExecutorServer。</p>
<p>要关闭AzkabanExecutorServer，请运行<code>bin/azkaban-exec-shutdown.sh</code>。</p>

            <hr>            
            <h2 id="multipleExecutors-config">配置多执行器模式的执行程序</h2>

<p>在这个时间点我们没有执行者管理界面。执行器需要在数据库中配置。举些例子:-</p>

<p>将所有执行程序插入到执行程序设置的mysql数据库中。验证executors表中正确的执行程序是否处于活动状态。
<code>insert into executors(host,port) values("EXECUTOR_HOST",EXECUTOR_PORT);</code></p>


            <hr>
            <h2 id="plugin-setup">设置Azkaban插件</h2>

Azkaban is designed to make non-core functionalities plugin-based, so that

<ol>
	<li>they can be selectively installed/upgraded in different environments without changing the core Azkaban, and</li>
	<li>it makes Azkaban very easy to be extended for different systems.</li>
</ol>

<p>Right now, Azkaban allows for a number of different plugins. On web server side, there are</p>
<ul>
	<li>viewer plugins that enable custom web pages to add features to Azkaban. Some of the known implementations include HDFS filesystem viewer, and Reportal.</li>
	<li>trigger plugins that enable custom triggering methods.</li>
	<li>user manager plugin that enables custom user authentication methods. For instance, in LinkedIn we have LDAP based user authentication.</li>
	<li>alerter plugins that enable different alerting methods to users, in addition to email based alerting.</li>
</ul>

<p>On executor server side</p>
<ul>
	<li>pluggable job type executors on AzkabanExecutorServer, such as job types for hadoop ecosystem components.</li>
</ul>

<p>We recommend installing these plugins for the best usage of Azkaban. A set of common plugins are available to download from the <a href="http://azkaban.github.io/downloads.html">download page</a>. Alternatively, by cloning the <a href="https://github.com/azkaban/azkaban-plugins">GitHub repo</a>, you can run <code>ant</code> in different plugin directories to create tar ball packages.</p>

<p>Below are instructions of how to install these plugins to work with Azkaban.</p>

<h3>User Manager Plugins</h3>
<p>By default, Azkaban ships with the XMLUserManager class which authenticates users based on a xml file, which is located at <code>conf/azkaban-users.xml</code>.</p>
<p>This is not secure and doesn't serve many users. In real production deployment, you should rely on your own user manager class that suits your need, such as a LDAP based one. The <code>XMLUserManager</code> can still be used for special user accounts and managing user roles. You can find examples of these two cases in the default <code>azkaban-users.xml</code> file.</p>

<p>To install your own user manager class, specify in <code>Azkaban2-web-server-install-dir/conf/azkaban.properties</code>:</p>

<pre>user.manager.class=MyUserManagerClass</pre>

<p>and put the containing jar in <code>plugins</code> directory.</p>

<h3>Viewer Plugins</h3>

<h4>HDFS Viewer Plugins</h4>

<p>HDFS Viewer Plugin should be installed in AzkabanWebServer plugins directory, which is specified in AzkabanWebServer's config file, for example, in <code>Azkaban2-web-server-install-dir/conf/azkaban.properties</code>:</p>

<pre>viewer.plugins=hdfs</pre>

<p>This tells Azkaban to load hdfs viewer plugin from <code>Azkaban2-web-server-install-dir/plugins/viewer/hdfs</code>.</p>

<p>Extract the <code>azkaban-hdfs-viewer</code> archive to the AzkabanWebServer <code>./plugins/viewer</code> directory. Rename the directory to <code>hdfs</code>, as specified above.</p>

<p>Depending on if the hadoop installation is turned on:</p>
<ol>
  <li>If the Hadoop installation does not have security turned on, the default config is good enough. One can simply restart <code>AzkabanWebServer</code> and start using the HDFS viewer.</li>
	<li>If the Hadoop installation does have security turned on, the following configs should be set differently than their default values, in plugin's config file:</li>
</ol>

<table class="table table-bordered table-condensed table-striped">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>azkaban.should.proxy</code></td>
			<td>Whether Azkaban should proxy as another user to view the hdfs filesystem, rather than Azkaban itself, defaults to <code>true</code></td>
		</tr>
		<tr>
			<td><code>hadoop.security.manager.class</code></td>
			<td>The security manager to be used, which handles talking to secure hadoop cluster, defaults to <code>azkaban.security.HadoopSecurityManager_H_1_0</code> (for hadoop 1.x versions)</td>
		</tr>
		<tr>
			<td><code>proxy.user</code></td>
			<td>The Azkaban user configured with kerberos and hadoop. Similar to how oozie should be configured, for secure hadoop installations</td>
		</tr>
		<tr>
			<td><code>proxy.keytab.location</code></td>
			<td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified <code>proxy.user</code></td>
		</tr>
	</tbody>
</table>

<p>For more Hadoop security related information, see <a href="#hadoopsecuritymanager">HadoopSecurityManager</a></p>

<h3>Job Type Plugins</h3>

<p>Azkaban has a limited set of built-in job types to run local unix commands and simple java programs. In most cases, you will want to install additional job type plugins, for example, hadoopJava, Pig, Hive, VoldemortBuildAndPush, etc. Some of the common ones are included in azkaban-jobtype archive. Here is how to install:</p>

<p>Job type plugins should be installed with AzkabanExecutorServer's plugins directory, and specified in AzkabanExecutorServer's config file. For example, in <code>Azkaban2-exec-server-install-dir/conf/azkaban.properties</code>:</p>

<pre>azkaban.jobtype.plugin.dir=plugins/jobtypes</pre>

<p>This tells Azkaban to load all job types from <code>Azkaban2-exec-server-install-dir/plugins/jobtypes</code>. Extract the archive into AzkabanExecutorServer <code>./plugins/</code> directory, rename it to <code>jobtypes</code> as specified above.</p>

<p>The following setting is often needed when you run Hadoop Jobs:</p>

<table class="table table-bordered table-condensed table-striped">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>hadoop.home</code></td>
			<td>Your <code>$HADOOP_HOME</code> setting.</td>
		</tr>
		<tr>
			<td><code>jobtype.global.classpath</code></td>
			<td>The cluster specific hadoop resources, such as hadoop-core jar, and hadoop conf (e.g. <code>${hadoop.home}/hadoop-core-1.0.4.jar,${hadoop.home}/conf</code>)</td>
		</tr>
	</tbody>
</table>

Depending on if the hadoop installation is turned on:

<ul>
  <li>If the hadoop installation does not have security turned on, you can likely rely on the default settings.</li>
  <li>If the Hadoop installation does have kerberos authentication turned on, you need to fill out the following hadoop settings:</li>
</ul>

<table class="table table-bordered table-condensed table-striped">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>hadoop.security.manager.class</code></td>
			<td>The security manager to be used, which handles talking to secure hadoop cluster, defaults to <code>azkaban.security.HadoopSecurityManager_H_1_0</code> (for hadoop 1.x versions)</td>
		</tr>
		<tr>
			<td><code>proxy.user</code></td>
			<td>The Azkaban user configured with kerberos and hadoop. Similar to how oozie should be configured, for secure hadoop installations</td>
		</tr>
		<tr>
			<td><code>proxy.keytab.location</code></td>
			<td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
		</tr>
	</tbody>
</table>

<p>For more Hadoop security related information, see <a href="#hadoopsecuritymanager">HadoopSecurityManager</a></p>

Finally, start the executor, watch for error messages and check executor server log. For job type plugins, the executor should do minimum testing and let you know if it is properly installed.

            <hr>
            <h2 id="property-override">Property Overrides</h2>

<p> Azkaban job is specified with a set of key-value pairs we call properties. There are multiple sources for deciding which properties will finally be a part of job execution. Following table lists out all the sources of properties and their priorities. Please note that if a property occur in multiple sources, then its value from high property source will be used </p>

<p>
	Following properties are visible to the users. These are the same properties which are merged to form <code>jobProps</code> in <code>AbstractProcessJob.java</code>
</p>
<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>PropertySource</th>
			<th>Description</th>
			<th>Priority</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>global.properties</code> in <code>conf</code> directory</td>
			<td>These are admin configured properties during Azkaban setup. Global to all jobtypes.</td>
			<td>Lowest (0)</td>
		</tr>
		<tr>
			<td><code>common.properties</code> in <code>jobtype</code> directory</td>
			<td>These are admin configured properties during Azkaban setup. Global to all jobtypes.</td>
			<td>1</td>
		</tr>
		<tr>
			<td><code>plugin.properties</code> in <code>jobtype/{jobtype-name}</code> directory</td>
			<td>These are admin configured properties during Azkaban setup. Restricted to a specific jobtype.</td>
			<td>2</td>
		</tr>
		<tr>
			<td><code>common.properties</code> in project zip</td>
			<td>These are user specified property which apply to all jobs in sibling or descendent directories</td>
			<td>3</td>
		</tr>
		<tr>
			<td>Flow properties specified while triggering flow execution</td>
			<td>These are user specified property. These can be specified from UI or Ajax call but cannot be saved in project zip.</td>
			<td>4</td>
		</tr>
		<tr>
			<td><code>{job-name}.job</code> job specification</td>
			<td>These are user specified property in actual job file</td>
			<td>Highest (5)</td>
		</tr>
	</tbody>
</table>

<p>
	Following properties are not visible to the users. Depending on jobtype implementation these properties are used for constraining user jobs and properties. These are the same properties which are merged to form <code>sysProps</code> in <code>AbstractProcessJob.java</code>
</p>
<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>PropertySource</th>
			<th>Description</th>
			<th>Priority</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>commonprivate.properties</code> in <code>jobtype</code> directory</td>
			<td>These are admin configured properties during Azkaban setup. Global to all jobtypes.</td>
			<td>Lowest (0)</td>
		</tr>
		<tr>
			<td><code>private.properties</code> in <code>jobtype/{jobtype-name}</code> directory</td>
			<td>These are admin configured properties during Azkaban setup. Restricted to a specific jobtype.</td>
			<td>Highest (1)</td>
		</tr>
	</tbody>
</table>

<p> 
	<code>azkaban.properties</code> is another type of properties which are only used for controlling Azkaban webserver and execserver configuration. Please note that <code>jobProps</code>, <code>sysProps</code> and <code>azkaban.properties</code> are 3 different types of properties and are not merged in general (depends on jobtype implementation).
</p>

            <hr>
            <h2 id="upgrade-21">Upgrading DB from 2.1</h2>

<p class="lead">If installing Azkaban from scratch, you can ignore this document. This is only for those who are upgrading from 2.1 to 2.5.</p>

<p>The <code>update_2.1_to_3.0.sql</code> needs to be run to alter all the tables. This includes several table alterations and a new table.</p>

<p>Here are the changes:</p>

<ul>
	<li>
		Alter project_properties table'
		<ul>
			<li>Modify 'name' column to be 255 characters</li>
		</ul>
	</li>
	<li>Create new table triggers</li>
</ul>

<h3>Importing Existing Schedules from 2.1</h3>

<p>In 3.0, the scheduling system is merged into the new triggering system. The information will be persisted in <code>triggers</code> table in DB. We have a simple tool to import your existing schedules into this new table.</p>

<p>After you download and install web server, please run this command <strong>once</strong> from web server install directory:</p>

<pre>
$ bash bin/schedule2trigger.sh
</pre>

            <hr>
            <h2 id="upgrade-27">Upgrading DB from 2.7.0</h2>

<p class="lead">If installing Azkaban from scratch, you can ignore this document. This is only for those who are upgrading from 2.7 to 3.0.</p>

<p>The <code>create.executors.sql</code>, <code>update.active_executing_flows.3.0.sql</code>, <code>update.execution_flows.3.0.sql</code>, and <code>create.executor_events.sql</code> needs to be run to alter all the tables. This includes several table alterations and two new table.</p>

<p>Here are the changes:</p>

<ul>
	<li>
		Alter active_executing_flows table'
		<ul>
			<li>Deleting 'port' column</li>
			<li>Deleting 'host' column</li>
		</ul>
	</li>
	<li>
		Alter execution_flows table'
		<ul>
			<li>Adding an 'executor_id' column</li>
		</ul>
	</li>
	<li>Create new executors table</li>
	<li>Create new executor events table</li>
</ul>

					</div>

					<div class="docs-section">
						<div class="page-header">
	<h1 id="configuration">Configuration</h1>
</div>

<p class="lead">Azkaban can be configured in many ways. The following describes the knobs and switches that can be set. For the most part, the there is no need to deviate from the default values.</p>

            <hr>
						<h2 id="azkaban-webserver">Azkaban Web Server Configurations</h2>

<p>These are properties to configure the web server. They can be set in <code>azkaban.properties</code>.</p>

<h3>General Properties</h3>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>azkaban.name</code></td>
			<td>The name of the azkaban instance that will show up in the UI. Useful if you run more than one Azkaban instance.</td>
			<td>Local</td>
		</tr>
		<tr>
			<td><code>azkaban.label</code></td>
			<td>A label to describe the Azkaban instance.</td>
			<td>My Local Azkaban</td>
		</tr>
		<tr>
			<td><code>azkaban.color</code></td>
			<td>Hex value that allows you to set a style color for the Azkaban UI.</td>
			<td>#FF3601</td>
		</tr>
		<tr>
			<td><code>web.resource.dir</code></td>
			<td>Sets the directory for the ui’s css and javascript files.</td>
			<td>web/</td>
		</tr>
		<tr>
			<td><code>default.timezone</code></td>
			<td>The timezone that will be displayed by Azkaban.</td>
			<td>America/Los_Angeles</td>
		</tr>
		<tr>
			<td><code>viewer.plugin.dir</code></td>
			<td>Directory where viewer plugins are installed.</td>
			<td>plugins/viewer</td>
		</tr><tr>
			<td><code>job.max.Xms</code></td>
			<td>The maximum initial amount of memory each job can request.  This validation is performed at project upload time</td>
			<td>1GB</td>
		</tr>
		<tr>
			<td><code>job.max.Xmx</code></td>
			<td>The maximum amount of memory each job can request.  This validation is performed at project upload time</td>
			<td>2GB</td>
		</tr>
	</tbody>
</table>

<h3>Multiple Executor Mode Parameters</h3>

<table class="table table-striped table-bordered table-condensed">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>azkaban.use.multiple.executors</td>
			<td>Should azkaban run in multi-executor mode. Required for multiple executor mode.</td>
			<td>false</td>
		</tr>
		<tr>
			<td>azkaban.executorselector.filters</td>
			<td>A common separated list of hard filters to be used while dispatching. To be choosen from StaticRemaining, FlowSize, MinimumFreeMemory and CpuStatus. Order of filter do not matter.</td>
			<td> </td>
		</tr>
		<tr>
			<td>azkaban.executorselector.comparator.{ComparatorName}</td>
			<td>Integer weight to be used to rank available executors for a given flow. Currently, {ComparatorName} can be NumberOfAssignedFlowComparator, Memory, LastDispatched and CpuUsage as ComparatorName. For example:- azkaban.executorselector.comparator.Memory=2</td>
			<td> </td>
		</tr>
		<tr>
			<td>azkaban.queueprocessing.enabled</td>
			<td>Hhould queue processor be enabled from webserver initialization</td>
			<td>true</td>
		</tr>
		<tr>
			<td>azkaban.webserver.queue.size</td>
			<td>Maximum flows that can be queued at webserver</td>
			<td>100000</td>
		</tr>
		<tr>
			<td>azkaban.activeexecutor.refresh.milisecinterval</td>
			<td>Maximum time in milliseconds that can be processed without executor statistics refresh</td>
			<td>50000</td>
		</tr>
		<tr>
			<td>azkaban.activeexecutor.refresh.flowinterval</td>
			<td>Maximum number of queued flows that can be processed without executor statistics refresh</td>
			<td>5</td>
		</tr>
		<tr>
			<td>azkaban.executorinfo.refresh.maxThreads</td>
			<td>Maximum number of threads to refresh executor statistics</td>
			<td>5</td>
		</tr>
	</tbody>
</table>

<h3>Jetty Parameters</h3>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>jetty.maxThreads</td>
			<td>Max request threads</td>
			<td>25</td>
		</tr>
		<tr>
			<td>jetty.ssl.port</td>
			<td>The ssl port</td>
			<td>8443</td>
		</tr>
		<tr>
			<td>jetty.keystore</td>
			<td>The keystore file</td>
			<td></td>
		</tr>
		<tr>
			<td>jetty.password</td>
			<td>The jetty password</td>
			<td></td>
		</tr>
		<tr>
			<td>jetty.keypassword</td>
			<td>The keypassword</td>
			<td></td>
		</tr>
		<tr>
			<td>jetty.truststore</td>
			<td>The trust store</td>
			<td></td>
		</tr>
		<tr>
			<td>jetty.trustpassword</td>
			<td>The trust password</td>
			<td></td>
		</tr>
	</tbody>
</table>

<h3>Project Manager Settings</h3>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>project.temp.dir</td>
			<td>The temporary directory used when uploading projects</td>
			<td>temp</td>
		</tr>
		<tr>
			<td>project.version.retention</td>
			<td>The number of unused project versions retained before cleaning</td>
			<td>3</td>
		</tr>
		<tr>
			<td>creator.default.proxy</td>
			<td>Auto add the creator of the projects as a proxy user to the project.</td>
			<td>true</td>
		</tr>
		<tr>
			<td>lockdown.create.projects</td>
			<td>Prevents anyone except those with Admin roles to create new projects.</td>
			<td>false</td>
		</tr>
		<tr>
			<td>lockdown.upload.projects</td>
			<td>Prevents anyone but admin users and users with permissions to upload
				projects.
			</td>
			<td>false</td>
		</tr>
	</tbody>
</table>

<h3>MySQL Connection Parameter</h3>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th> Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>database.type</td>
			<td>The database type. Currently, the only database supported is mysql.</td>
			<td>mysql</td>
		</tr>
		<tr>
			<td>mysql.port</td>
			<td>The port to the mysql db</td>
			<td>3306</td>
		</tr>
		<tr>
			<td>mysql.host</td>
			<td>The mysql host</td>
			<td>localhost</td>
		</tr>
		<tr>
			<td>mysql.database</td>
			<td>The mysql database</td>
			<td></td>
		</tr>
		<tr>
			<td>mysql.user</td>
			<td>The mysql user</td>
			<td></td>
		</tr>
		<tr>
			<td>mysql.password</td>
			<td>The mysql password</td>
			<td></td>
		</tr>
		<tr>
			<td>mysql.numconnections</td>
			<td>The number of connections that Azkaban web client can open to the database</td>
			<td>100</td>
		</tr>
	</tbody>
</table>

<h3>Executor Manager Properties</h3>

<table class="table table-striped table-bordered table-condensed">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>executor.port</td>
			<td>The port for the azkaban executor server</td>
			<td>12321</td>
		</tr>
		<tr>
			<td>executor.host</td>
			<td>The host for azkaban executor server</td>
			<td>localhost</td>
		</tr>
		<tr>
			<td>execution.logs.retention.ms</td>
			<td>Time in milliseconds that execution logs are retained</td>
			<td>7257600000L (12 weeks)</td>
		</tr>
	</tbody>
</table>

<h3>Notification Email Properties</h3>

<table class="table table-striped table-bordered table-condensed">
	<thead>
		<tr>
			<th>Parameter</th>
			<th> Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>mail.sender</td>
			<td>The email address that azkaban uses to send emails.</td>
			<td></td>
		</tr>
		<tr>
			<td>mail.host</td>
			<td>The email server host machine.</td>
			<td></td>
		</tr>
		<tr>
			<td>mail.user</td>
			<td>The email server user name.</td>
			<td></td>
		</tr>
		<tr>
			<td>mail.password</td>
			<td>The email password user name.</td>
			<td></td>
		</tr>
	</tbody>
</table>

<h3>User Manager Properties</h3>

<table class="table table-striped table-bordered table-condensed">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>user.manager.class</td>
			<td>The user manager that is used to authenticate a user. The default is an XML user manager, but it can be overwritten to support other authentication methods, such as JDNI.</td>
			<td>azkaban.user.XmlUserManager</td>
		</tr>
		<tr>
			<td>user.manager.xml.file</td>
			<td>Xml file for the XmlUserManager</td>
			<td>conf/azkaban-users.xml</td>
		</tr>
	</tbody>
</table>

<h3>User Session Properties</h3>

<table class="table table-striped table-bordered table-condensed">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>session.time.to.live</td>
			<td>The session time to live in ms seconds</td>
			<td>86400000</td>
		</tr>
		<tr>
			<td>max.num.sessions</td>
			<td>The maximum number of sessions before people are evicted.</td>
			<td>10000</td>
		</tr>
	</tbody>
</table>

            <hr>
						
<h2 id="azkaban-execserver">Azkaban Executor Server Configuration</h2>

<h3>Executor Server Properties</h3>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>executor.port</code></td>
			<td>The port for azkaban executor server</td>
			<td>12321</td>
		</tr>
		<tr>
			<td><code>executor.global.properties</code></td>
			<td>A path to the properties that will be the parent for all jobs.</td>
			<td><code>none</code></td>
		</tr>
		<tr>
			<td><code>azkaban.execution.dir</code></td>
			<td>The folder for executing working directories</td>
			<td>executions</td>
		</tr>
		<tr>
			<td><code>azkaban.project.dir</code></td>
			<td>The folder for storing temporary copies of project files used for executions</td>
			<td>projects</td>
		</tr>
		<tr>
			<td><code>executor.flow.threads</code></td>
			<td>The number of simulateous flows that can be run. These threads are mostly idle.</td>
			<td>30</td>
		</tr>
		<tr>
			<td><code>job.log.chunk.size</code></td>
			<td>For rolling job logs. The chuck size for each roll over</td>
			<td>5MB</td>
		</tr>
		<tr>
			<td><code>job.log.backup.index</code></td>
			<td>The number of log chunks. The max size of each logs is then the index * chunksize</td>
			<td>4</td>
		</tr>
		<tr>
			<td><code>flow.num.job.threads</code></td>
			<td>The number of concurrent running jobs in each flow. These threads are mostly idle.</td>
			<td>10</td>
		</tr>
		<tr>
			<td><code>job.max.Xms</code></td>
			<td>The maximum initial amount of memory each job can request.  If a job requests more than this, then Azkaban server will not launch this job</td>
			<td>1GB</td>
		</tr>
		<tr>
			<td><code>job.max.Xmx</code></td>
			<td>The maximum amount of memory each job can request.  If a job requests more than this, then Azkaban server will not launch this job</td>
			<td>2GB</td>
		</tr>
		<tr>
			<td><code>azkaban.server.flow.max.running.minutes</code></td>
			<td>The maximum time in minutes a flow will be living inside azkaban after being executed.  If a flow runs longer than this, it will be killed. If smaller or equal to 0, there's no restriction on running time.</td>
			<td>-1</td>
		</tr>
	</tbody>
</table>

<h3>MySQL Connection Parameter</h3>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>database.type</code></td>
			<td>The database type. Currently, the only database supported is mysql.</td>
			<td>mysql</td>
		</tr>
		<tr>
			<td><code>mysql.port</code></td>
			<td>The port to the mysql db</td>
			<td>3306</td>
		</tr>
		<tr>
			<td><code>mysql.host</code></td>
			<td>The mysql host</td>
			<td>localhost</td>
		</tr>
		<tr>
			<td><code>mysql.database</code></td>
			<td>The mysql database</td>
			<td></td>
		</tr>
		<tr>
			<td><code>mysql.user</code></td>
			<td>The mysql user</td>
			<td></td>
		</tr>
		<tr>
			<td><code>mysql.password</code></td>
			<td>The mysql password</td>
			<td></td>
		</tr>
		<tr>
			<td><code>mysql.numconnections</code></td>
			<td>The number of connections that Azkaban web client can open to the database</td>
			<td>100</td>
		</tr>
	</tbody>
</table>

            <hr>
						
<h2 id="azkaban-plugin-configuration">Plugin Configurations</h2>

<h3>Execute-As-User</h3>

<p>With a new security enhancement in Azkaban 3.0, Azkaban jobs can now run as the submit user or the user.to.proxy of the flow by default.  This ensures that Azkaban takes advantage of the Linux permission security mechanism, and operationally this simplifies resource monitoring and visibility.  Set up this behavior by doing the following:-</p>

<ol>
	<li>Execute.as.user is set to true by default. In case needed, it can also be configured to false in azkaban-plugin’s commonprivate.properties</li>
	<li>Configure azkaban.native.lib= to the place where you are going to put the compiled execute-as-user.c file (see below)</li>
	<li>Generate an executable on the Azkaban box for azkaban-common/src/main/c/execute-as-user.c. <b> it should be named execute-as-user </b> Below is a sample approach</li>
		<ul> 
			<li><code>scp ./azkaban-common/src/main/c/execute-as-user.c</code> onto the Azkaban box</li>
			<li>run: <code>gcc execute-as-user.c -o execute-as-user</code></li>
			<li>run: <code>chown root execute-as-user (you might need root privilege)</code></li>
			<li>run: <code>chmod 6050 execute-as-user (you might need root privilege)</code></li>
		</ul>
	</li>
</ol>

					</div>

					<div class="docs-section">
						<div class="page-header">
	<h1 id="user-manager">UserManager</h1>
</div>

<p>When you start Azkaban, you may notice the login page. Azkaban makes you authenticate before you can use it. This is prevent seeing or executing workflows you shoudn't see or touch.</p>
<p>We also used authenticated users for auditing purposes. Whenever project files change, is modified, scheduled, etc. we often want to know which user performed that action.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/login.png" data-lightbox="login" title="Login">
      <img class="img-thumbnail img-figure" title="Azkaban Login" src="./images/login.png" alt="Azkaban Login" width="400">
    </a>
  </div>
</div>

            <hr>
						<h2 id="xml-usermanager">XmlUserManager</h2>

<p>The XmlUserManager is the default UserManager that is built into Azkaban. To explicitly set the parameters that configure the XmlUserManager, the following parameters can be set in the <code>azkaban.properties</code> file.</p>

<table class="table table-bordered table-condensed table-striped">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>user.manager.class</td>
			<td>azkaban.user.XmlUserManager</td>
		</tr>
		<tr>
			<td>user.manager.xml.file</td>
			<td>azkaban-users.xml</td>
		</tr>
	</tbody>
</table>

<p>The other file that needs to be modified is the <code>azkaban-users.xml</code> file. The XmlUserManager will parse the user xml file once during startup to set up the users.</p>

<p>Everything must be enclosed in a <code>&lt;azkaban-users&gt;</code> tag.</p>

<pre class="code">
&lt;azkaban-users&gt;
	...
&lt;/azkaban-users&gt;
</pre>

<h3>Users</h3>

<p>To add users, add the <code>&lt;user&gt;</code> tag.</p>

<pre class="code">
&lt;azkaban-users&gt;
  &lt;user username="myusername" password="mypassword" roles="a" groups="mygroup" / &gt;
  &lt;user username="myusername2" password="mypassword2" roles="a, b" groups="ga, gb" / &gt;
  ...
&lt;/azkaban-users&gt;
</pre>

<table class="table table-striped table-bordered table-condensed">
	<thead>
		<tr>
			<th>Attributes</th>
			<th>Values</th>
			<th>Required?</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>username</td>
			<td>The login username.</td>
			<td>yes</td>
		</tr>
		<tr>
			<td>password</td>
			<td>The login password.</td>
			<td>yes</td>
		</tr>
		<tr>
			<td>roles</td>
			<td>Comma delimited list of roles that this user has.</td>
			<td>no</td>
		</tr>
		<tr>
			<td>groups</td>
			<td>Comma delimited list of groups that the users belongs to.</td>
			<td>no</td>
		</tr>
		<tr>
			<td>proxy</td>
			<td>Comma delimited list of proxy users that this users can give to a project</td>
			<td>no</td>
		</tr>
	</tbody>
</table>

<h3>Groups</h3>

<p>To define each group, you can add the <code>&lt;group&gt;</code> tag.</p>

<pre class="code">
&lt;azkaban-users&gt;
  &lt;user username="a" ... groups="groupa" / &gt;
  ...
  &lt;group name="groupa" roles="myrole" / &gt;
  ...
&lt;/azkaban-users&gt;
</pre>

<p>In the previous example, user 'a' is in the group 'groupa'. User 'a' would also have the 'myrole' role. A regular user cannot add group permissions to a project unless they are members of that group.</p>

<p>The following are some group attributes that you can assign.</p>

<table class="table table-condensed table-striped table-bordered">
	<thead>
		<tr>
			<th>Attributes</th>
			<th>Values</th>
			<th>Required?</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>name</td>
			<td>The group name</td>
			<td>yes</td>
		</tr>
		<tr>
			<td>roles</td>
			<td>Comma delimited list of roles that this user has.</td>
			<td>no</td>
		</tr>
	</tbody>
</table>

<h3>Roles</h3>

<p>Roles are different in that it assigns global permissions to users in Azkaban. You can set up roles with the <code>&lt;roles&gt;</code> tag.</p>

<pre class="code">
&lt;azkaban-users&gt;
  &lt;user username="a" ... groups="groupa" roles="readall" / &gt;
  &lt;user username="b" ... / &gt;
  ...
  &lt;group name="groupa" roles="admin" / &gt
  ...
  &lt;role name="admin" permissions="ADMIN" / &gt;
  &lt;role name="readall" permissions="READ" / &gt;
&lt;/azkaban-users&gt;
</pre>

<p>In the above example, user 'a' has the role 'readall', which is defined as having the READ permission. This means that user 'a' has global READ access on all the projects and executions.</p>
<p>User 'a' also is in 'groupa', which has the role ADMIN. It's certainly redundant, but user 'a' is also granted the ADMIN role on all projects.</p>

<p>The following are some group attributes that you can assign.</p>

<table class="table table-condensed table-bordered table-striped">
	<thead>
		<tr>
			<th>Attributes</th>
			<th>Values</th>
			<th>Required?</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>name</td>
			<td>The group name</td>
			<td>yes</td>
		</tr>
		<tr>
			<td>permissions</td>
			<td>Comma delimited list global permissions for the role</td>
			<td>yes</td>
		</tr>
	</tbody>
</table>

<p>The possible role permissions are the following:</p>

<table class="table table-condensed table-bordered table-striped">
	<thead>
		<tr>
			<th>Permissions</th>
			<th>Values</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>ADMIN</td>
			<td>Grants all access to everything in Azkaban.</td>
		</tr>
		<tr>
			<td>READ</td>
			<td>Gives users read only access to every project and their logs</td>
		</tr>
		<tr>
			<td>WRITE</td>
			<td>Allows users to upload files, change job properties or remove any project</td>
		</tr>
		<tr>
			<td>EXECUTE</td>
			<td>Allows users to trigger the execution of any flow</td>
		</tr>
		<tr>
			<td>SCHEDULE</td>
			<td>Users can add or remove schedules for any flows</td>
		</tr>
		<tr>
			<td>CREATEPROJECTS</td>
			<td>Allows users to create new projects if project creation is locked down</td>
		</tr>
	</tbody>
</table>

            <hr>
						<h2 id="custom-usermanager">Custom User Manager</h2>

<p>Although the XmlUserManager is easy enough to get started with, you may want to integrate with an already established directory system, such as LDAP.</p>

<p>It should be fairly straight forward to implement a custom UserManager. The UserManager is a java interface. There are only a few methods needed to implement.</p>

<pre class="code">
public interface UserManager {
	public User getUser(String username, String password) throws UserManagerException;
	public boolean validateUser(String username);
	public boolean validateGroup(String group);
	public Role getRole(String roleName);
	public boolean validateProxyUser(String proxyUser, User realUser);
}
</pre>

<p>The constructor should take an <code>azkaban.utils.Props</code> object. The contents of <code>azkaban.properties</code> will be available for the UserManager for configuration.</p>

<p>Package your new custom UserManager into a jar and drop it into the <code>./extlib</code> directory or alternatively into the plugins directory (i.e. <code>./plugins/ldap/linkedin-ldap.jar</code>).</p>

<p>Change the <code>azkaban.properties</code> configuration to point to the custom UserManager. Add additional parameters into <code>azkaban.properties</code> if needed by your custom user manager.</p>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Default</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>user.manager.class</code></td>
			<td><code>azkaban.user.CustomUserManager</code></td>
		</tr>
	</tbody>
</table>


					</div>

					<div class="docs-section">
						<div class="page-header">
	<h1 id="creating-flows">创建流程</h1>
</div>

<p>作业是你想在Azkaban运行的过程。作业可以设置为依赖于其他作业。由一组作业及其依赖关系创建的图形构成了一个流程。</p>

<h3>创建作业</h3>

<p>创建作业非常简单。我们创建一个<code>.job</code>扩展属性文件。此作业文件定义了要运行的作业类型，依赖性以及正确设置作业所需的任何参数。</p>

<pre class="code">
# foo.job
type=command
command=echo "Hello World"
</pre>

<p>在这个例子中，作业的<code>type</code>是<code>command</code>。该<code>command</code>参数是<code>command</code>类型作业可以理解的参数。在这种情况下，它将运行命令来打印“Hello World”。将标准输出和作业的标准错误写入日志，日志可在Azkaban Web UI中查看。</p>

<P>有关作业的更多信息，请参阅<a href="#job-configuration">作业配置</a> 页面。</p>

<h3>创建流程</h3>

<p>流程是一组相互依赖的作业。作业的依赖始终在作业本身运行之前运行。要向作业中添加依赖项<code>dependencies</code>，请按以下示例中所示来添加该属性。</p>

<pre class="code">
# foo.job
type=command
command=echo foo
</pre>

<pre class="code">
# bar.job
type=command
dependencies=foo
command=echo bar
</pre>

<p>该<code>dependencies</code>参数采用逗号分隔的作业名称列表。确保作业名称存在，并且没有循环依赖。</p>

<p>为每个没有作业依赖的作业创建一个流程，并为作业流程赋予相同的名称。例如，在上面的例子中，<code>bar</code>依赖于<code>foo</code>，但没有任何依赖<code>bar</code>。因此将会创建一个名称为<code>bar</code>流程。</p>

<h3>嵌入式流程</h3>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/embedded-flow.png" data-lightbox="embedded-flow" title="Embedded flows">
      <img src="./images/embedded-flow.png" class="img-thumbnail img-figure" alt="Embedded flow" width="400" />
    </a>
  </div>
</div>

<p>流程也以<strong>嵌入式流程</strong>形式作为其他流程中的节点，被包含进其他流程中。要创建嵌入式流程，只需创建一个<code>.job</code>文件，其<code>type=flow</code>并将其<code>flow.name</code>设置为流程名称。例如：</p>

<pre class="code">
# baz.job
type=flow
flow.name=bar
</pre>
<p>同样可以，通过给每个嵌入式流程的<code>.job</code>文件添加参数，实现不同设置，一起被嵌入到其他流程中。</p>

<h3>上传流程</h3>

<p>要上传流程，只需要将<code>.job</code>和所有需要执行的二进制文件存档到一个<code>.zip</code>文件中。通过Azkaban UI，您就可以部署您的作业流程。该流程将针对缺失或循环依赖性进行验证。查看<a href="#project-uploads">项目上传</a>。</p>

            <hr>
						<h2 id="job-configuration">Job Configurations</h2>

<h3>Common Parameters</h3>

<p>Besides the <code>type</code> and the <code>dependencies</code> parameters, there are several parameters that Azkaban reserves for all jobs. All of the parameters below are optional.</p>

<table class="table table-bordered table-condensed table-striped">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>retries</td>
			<td>The number of retries that will be automatically attempted for failed jobs</td>
		</tr>
		<tr>
			<td>retry.backoff</td>
			<td>The millisec time between each retry attempt</td>
		</tr>
		<tr>
			<td>working.dir</td>
			<td>Override the working directory for the execution. This is by default the directory that contains the job file that is being run.</td>
		</tr>
		<tr>
			<td>env.<em>property</em></td>
			<td>Set the environment variable with named <em>property</em></td>
		</tr>
		<tr>
			<td>failure.emails</td>
			<td>Comma delimited list of emails to notify during a failure. *</td>
		</tr>
		<tr>
			<td>success.emails</td>
			<td>Comma delimited list of emails to notify during a success. *</td>
		</tr>
		<tr>
			<td>notify.emails</td>
			<td>Comma delimited list of emails to notify during either a success or failure. *</td>
		</tr>
	</tbody>
</table>

<div class="bs-callout bs-callout-warning">
	<h4>Email properties</h4>
	<p>Note that for email properties, this property is retrieved from the last job in the flow and applied flow level. All other email properties of jobs in the flow are ignored.</p>
</div>


<h3>Runtime Properties</h3>

<p>These properties are automatically added to Azkaban properties during runtime for a job to use.</p>

<table class="table table-bordered table-striped table-condensed">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>azkaban.job.attempt</td>
			<td>The attempt number for the job. Starts with attempt 0 and increments with every retry.</td>
		</tr>
		<tr>
			<td>azkaban.job.id</td>
                        <td>The job name.</td>
                </tr>
                <tr>
			<td>azkaban.flow.flowid</td>
			<td>The flow name that the job is running in.</td>
		</tr>
		<tr>
			<td>azkaban.flow.execid</td>
			<td>The execution id that is assigned to the running flow.</td>
		</tr>
		<tr>
			<td>azkaban.flow.projectid</td>
			<td>The numerical project id.</td>
		</tr>
		<tr>
			<td>azkaban.flow.projectversion</td>
			<td>The project upload version.</td>
		</tr>
		<tr>
			<td>azkaban.flow.uuid</td>
			<td>A unique identifier assigned to a flow&#8217;s execution.</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.timestamp</td>
			<td>The millisecs since epoch start time.</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.year</td>
			<td>The start year.</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.month</td>
			<td>The start month of the year (1-12)</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.day</td>
			<td>The start day of the month.</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.hour</td>
			<td>The start hour in the day.</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.minute</td>
			<td>The start minute.</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.second</td>
			<td>The start second in the minute.</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.milliseconds</td>
			<td>The start millisec in the sec</td>
		</tr>
		<tr>
			<td>azkaban.flow.start.timezone</td>
			<td>The start timezone that is set.</td>
		</tr>
	</tbody>
</table>

<h3>Inherited Parameters</h3>

<p>Any included <code>.properties</code> files will be treated as properties that are shared amongst the individual jobs of the flow. The properties are resolved in a hierarchical manner by directory.</p>

<p>For instance, suppose you have the following directory structure in your zip file.</p>

<pre class="code">
system.properties
baz.job
myflow/
   myflow.properties
   myflow2.properties
   foo.job
   bar.job
</pre>

<p>That directory structure will be preserved when running in Azkaban. The <code>baz</code> job will inherit only from <code>system.properties</code>. The jobs <code>foo</code> and <code>bar</code> will inherit from <code>myflow.properties</code> and <code>myflow2.properties</code>, which in turn will inherit from <code>system.properties</code>.</p>

<p>The hierarchical ordering of properties in the same directory is arbitrary.</p>

<h3>Parameter Substitution</h3>

<p>Azkaban allows for replacing of parameters. Whenever a <code>${parameter}</code> is found in a properties or job file, Azkaban will attempt to replace that parameter. The resolution of the parameters is done late.</p>

<pre class="code">
# shared.properties
replaceparameter=bar
</pre>

<pre class="code">
# myjob.job
param1=mytest
foo=${replaceparameter}

param2=${param1}
</pre>

<p>In the previous example, before <code>myjob</code> is run, <code>foo</code> will equal <code>bar</code> and <code>param2</code> will equal <code>mytest</code>.</p>

<p>
A parameter cannot contain spaces. Following <code>invalid.job</code> is an example of incorrect parameter name.
</p>

<pre class="code">
# invalid.job
type=command
command=${wh oa mi}
command.1=${whoami }
</pre>

<h3>Parameter Passing</h3>

<p>There is often a desire to pass these parameters to the executing job code. The method of passing these parameters is dependent on the jobtype that is run, but usually Azkaban writes these parameters to a temporary file that is readable by the job.</p>

<p>The path of the file is set in <code>JOB_PROP_FILE</code> environment variable. The format is the same key value pair property files. Certain built-in job types do this automatically for you. The <code>java</code> type, for instance, will invoke your Runnable and given a proper constructor, Azkaban can pass parameters to your code automatically.</p>

<h3>Parameter Output</h3>

<p>Properties can be exported to be passed to its dependencies. A second environment variable <code>JOB_OUTPUT_PROP_FILE</code> is set by Azkaban. If a job writes a file to that path, Azkaban will read this file and then pass the output to the next jobs in the flow.</p>

<p>The output file should be in json format. Certain built-in job types can handle this automatically, such as the <code>java</code> type.</p>


            <hr>
						<h2 id="builtin-jobtypes">Built-in Job types</h2>

<p>Azkaban allows custom job types to be added as <a href="#plugins">plugins</a>. However it also supplies several built-in job types. On top of the job parameters that can be set, each job type has additional properties that can be used.</p>

<h3>Command</h3>

<p>Command type of job can be set with <code>type=command</code>. It is a barebones command line executor. Many of the other job types wrap the _command_ job type but constructs their own command lines.</p>

<table class="table table-condensed table-striped table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Required?</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>command</td>
			<td>The command line string to execute. i.e. <code>ls -lh</code></td>
			<td>yes</td>
		</tr>
		<tr>
			<td>command. <em>n</em></td>
			<td>Where <em>n</em> is a sequence of integers (i.e 1,2,3&#8230;). Defines additional commands that run in sequential order after the initial command.</td>
			<td>no</td>
		</tr>
	</tbody>
</table>

<h3>Java Process</h3>

<p>Java process jobs are a convenient wrapper for kicking off Java-based programs. It is equivalent to running a class with a main method from the command line. The following properties are available in javaprocess jobs: </p>

<table class="table table-condensed table-striped table-bordered">
	<thead>
		<tr>
			<th>Parameter</th>
			<th>Description</th>
			<th>Required?</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>java.class</td>
			<td>The class that contains the main function. i.e <code>azkaban.example.text.HelloWorld</code></td>
			<td>yes</td>
		</tr>
		<tr>
			<td>classpath</td>
			<td>Comma delimited list of jars and directories to be added to the classpath. Default is all jars in the current working directory.</td>
			<td>no</td>
		</tr>
		<tr>
			<td>Xms</td>
			<td>The initial memory pool start size. The default is 64M</td>
			<td>no</td>
		</tr>
		<tr>
			<td>Xmx</td>
			<td>The initial maximum memory pool size. The default is 256M</td>
			<td>no</td>
		</tr>
		<tr>
			<td>main.args</td>
			<td>A list of comma delimited arguments to pass to the java main function</td>
			<td>no</td>
		</tr>
		<tr>
			<td>jvm.args</td>
			<td>JVM args. This entire string is passed intact as a VM argument. <code>-Dmyprop=test -Dhello=world</code></td>
			<td>no</td>
		</tr>
	</tbody>
</table>

<h3>Noop</h3>

<p>A job that takes no parameters and is essentially a null operation. Used for organizing your graph.</p>


					</div>

					<div class="docs-section">
						<div class="page-header">
	<h1 id="using-azkaban">使用Azkaban</h1>
</div>

<p>本节介绍如何使用Azkaban Web UI创建，查看和执行您的流程。</p>


            <hr>
						<h2 id="create-projects">创建项目</h2>

<p>登录Azkaban后，您将看到Projects页面。此页面将显示您拥有读取权限的所有项目的列表。其中只有组权限的项目或具有READ或ADMIN角色的项目不会出现。</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/emptyprojectpage.png" data-lightbox="emptyprojectpage" title="Project page">
      <img class="img-thumbnail img-figure" title="Empty Project Page" src="./images/emptyprojectpage.png" alt="Project Page" width="400" />
    </a>
  </div>
</div>

<p>如果您刚刚开始，则项目页面可能为空。但是，您可以通过单击<strong>All Projects</strong>来查看所有现有项目。</p>

<p>点击<strong>Create Projects</strong>将弹出一个对话框。输入唯一的项目名称和项目描述。说明可以在将来更改，但项目名称不能。如果您没有看到此按钮，则除了具有适当权限的用户之外，创建新项目的功能可能已锁定。</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/createproject.png" data-lightbox="createproject" title="Create project">
      <img class="img-thumbnail img-figure" title="Create project" src="./images/createproject.png" alt="Create project" width="400" />
    </a>
  </div>
</div>

<p>创建项目后，会出现一个空白的项目页面。您将自动获得该项目的ADMIN状态。通过单击<a href="#project-permissions">Permissions</a>按钮添加和删​​除权限。</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/newprojectpage.png" data-lightbox="newprojectpage" title="New Project">
      <img class="img-thumbnail img-figure" title="New Project" src="./images/newprojectpage.png" alt="New Project" width="400" />
    </a>
  </div>
</div>

<p>如果您拥有适当的权限（如果创建项目，您应该使用该权限），则可以从此页面删除项目，更新说明，上载文件和查看项目日志。</p>

            <hr>
						
<h2 id="upload-projects">上传项目</h2>

<p>点击<strong>Upload</strong>按钮。你会看到下面的对话框。</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/uploadprojects.png" data-lightbox="upload-projects" title="Upload">
      <img class="img-thumbnail img-figure" title="Upload" src="./images/uploadprojects.png" alt="Upload" width="400" />
    </a>
  </div>
</div>

<p>选择您想要上传的工作流文件的存档文件。目前Azkaban只支持<code>*.zip</code>文件。该zip文件应该包含<code>*.job</code>文件以及运行作业所需的任何文件。作业名称在项目中必须是唯一的。</p>

<P>Azkaban将验证压缩文件的内容以确保符合依赖性，并且没有检测到循环依赖。如果发现任何无效流程，上传将失败。</p>

<p>上传会覆盖项目中的所有文件。上传新的zip文件后，对作业所做的任何更改都将被清除。</p>

<P>成功上传后，您应该可以看到屏幕上列出的所有流程。</p>

            <hr>
						<h2 id="flow-view">流程视图</h2>

<p>通过点击流程链接，您可以进入流程视图页面。从这里开始，您将看到流程的图形表示。左侧面板包含流程中的作业列表。</p>

<p>右键单击右侧面板中的作业或图形中的节点将允许您打开单个作业。您也可以在此页面Schedule和Execute流程。</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/flowview.png" data-lightbox="flow-view" title="Flow view">
      <img class="img-thumbnail" title="Flow View" src="./images/flowview.png" alt="Flow View" />
    </a>
  </div>
</div>

<p>点击Executions选项卡将显示此流程的所有先前执行情况。</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/flowviewexecutions.png" data-lightbox="flow-view" title="Flow executions">
      <img class="img-thumbnail" title="Flow Executions" src="./images/flowviewexecutions.png" alt="Flow Executions" />
    </a>
  </div>
</div>

            <hr>
						<h2 id="project-permissions">项目权限</h2>

<p>创建项目时，创建者将自动获得项目的ADMIN权限状态。这允许创建者查看，上传，更改作业，运行流程，删除和向项目添加用户权限。管理员可以删除其他管理员，但不能删除自己。这样可以防止当管理员全被具有管理员角色的用户删除后，项目不受管理。</p>

<p>权限页面可从项目页面访问。在权限页面上，管理员可以将其他用户，组或代理用户添加到项目中。</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/permission.png" data-lightbox="permission" title="Project permissions.">
      <img class="img-thumbnail img-figure" title="Permissions Page" src="./images/permission.png" alt="Permission" width="500" />
    </a>
  </div>
</div>

<ul>
	<li>添加用户权限为这些用户提供了项目的指定权限。通过取消选中所有权限来删除用户权限。</li>
	<li>组权限允许特定组中的每个人都拥有指定的权限。通过取消选中所有组权限来删除组权限。</li>
	<li>如果启用代理用户，代理用户允许项目工作流程以这些用户身份运行。这对于锁定哪些无头帐号作业可以代理来说很有用。一旦添加，点击“删除”按钮即可删除它们。</li>
</ul>

<p>每个用户都通过UserManager进行验证，以防止添加无效用户。组和代理用户也进行检查以确保它们是有效的，并查看是否允许管理员将它们添加到项目中</p>

<p>可以为用户和组设置以下权限：</p>

<table class="table table-striped table-condensed table-bordered">
	<thead>
		<tr>
			<th>权限</th>
			<th>描述</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>ADMIN</td>
			<td>允许用户对此项目执行任何操作，并添加权限并删除项目</td>
		</tr>
		<tr>
			<td>READ</td>
			<td>用户可以查看作业，流程和执行日志。</td>
		</tr>
		<tr>
			<td>WRITE</td>
			<td>项目文件可以上传，作业文件可以修改。</td>
		</tr>
		<tr>
			<td>EXECUTE</td>
			<td>用户被允许执行，暂停，取消作业。</td>
		</tr>
		<tr>
			<td>SCHEDULE</td>
			<td>用户可以添加，修改和删除日程表中的流程。</td>
		</tr>
	</tbody>
</table>

            <hr>
						<h2 id="executing-flows">执行流程</h2>

从<a href="#flow-view">流程视图</a>或项目页面中，您可以触发要执行的作业。您将看到一个正在执行的面板弹出窗口。

<h3>执行流程视图</h3>

在“流程视图”面板中，可以右键单击图并禁用或启用作业。在执行期间，被禁用的作业将被跳过，就好像它们的依赖关系已被满足一样。被禁用的作业将呈现半透明状态。

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executeflowpanel.png" data-lightbox="execute-flow" title="Executing flow view">
      <img class="img-thumbnail img-figure" title="Executing Flow View" src="./images/executeflowpanel.png" alt="Executing Flow View" width="450" />
    </a>
  </div>
</div>

<h3>Notification Options</h3>

<p>The notification options allow users to change the flow's success or failure notification behavior.</p>

<h4>Notify on Failure</h4>

<ul>
	<li><strong>First Failure</strong> - Send failure emails after the first failure is detected.</li>
	<li><strong>Flow Finished</strong> - If the flow has a job that has failed, it will send failure emails after all jobs in the flow have finished.</li>
</ul>

<h4>Email overrides</h4>

<p>Azkaban will use the default notification emails set in the final job in the flow. If overridden, a user can change the email addresses where failure or success emails are sent. The list can be delimited by commas, whitespace or a semi-colon.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executeflownotify.png" data-lightbox="execute-flow" title="Notification Options">
      <img class="img-thumbnail img-figure" title="Notification Options" src="./images/executeflownotify.png" ALT="Notification Options" width="450" />
    </a>
  </div>
</div>

<h3>Failure Options</h3>

<p>When a job in a flow fails, you are able to control how the rest of the flow will succeed.</p>

<ul>
  <li><strong>Finish Current Running</strong> will finish the jobs that are currently running, but it will not start new jobs. The flow will be put in the <code>FAILED FINISHING</code> state and be set to FAILED once everything completes.</li>
	<li><strong>Cancel All</strong> will immediately kill all running jobs and set the state of the executing flow to FAILED.</li>
  <li><strong>Finish All Possible</strong> will keep executing jobs in the flow as long as its dependencies are met. The flow will be put in the <code>FAILED FINISHING</code> state and be set to FAILED once everything completes.</li>
</ul>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executeflowfailure.png" data-lightbox="execute-flow" title="Failure options">
      <img class="img-thumbnail img-figure" title="Failure Options" src="./images/executeflowfailure.png" ALT="Failure Options" width="450" />
    </a>
  </div>
</div>

<h3>Concurrent Options</h3>

<p>If the flow execution is invoked while the flow is concurrently executing, several options can be set.</p>

<ul>
	<li><strong>Skip Execution</strong> option will not run the flow if its already running.</li>
	<li><strong>Run Concurrently</strong> option will run the flow regardless of if its running. Executions are given different working directories.</li>
	<li>
		<strong>Pipeline</strong> runs the the flow in a manner that the new execution will not overrun the concurrent execution.
		<ul>
      <li>Level 1: blocks executing <strong>job A</strong> until the the previous flow's <strong>job A</strong> has completed.</li>
      <li>Level 2: blocks executing <strong>job A</strong> until the the children of the previous flow's <strong>job A</strong> has completed. This is useful if you need to run your flows a few steps behind an already executin flow.</li>
		</ul>
	</li>
</ul>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executeflowconcurrent.png" data-lightbox="execute-flow" title="Concurrent options">
      <img class="img-thumbnail img-figure" title="Concurrent Options" src="./images/executeflowconcurrent.png" ALT="Concurrent Options" width="450" />
    </a>
  </div>
</div>

<h3>Flow Parameters</h3>

<p>Allows users to override flow parameters. The flow parameters override the global properties for a job, but not the properties of the job itself.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executeflowparameters.png" data-lightbox="execute-flow-panel" title="Flow parameters">
      <img class="img-thumbnail img-figure" title="Flow Parameters Options" src="./images/executeflowparameters.png" ALT="Flow Parameters" width="450" />
    </a>
  </div>
</div>

            <hr>
						<h2 id="executions">Executions</h2>

<h3>Flow Execution Page</h3>

<p>After <a href="#executing-flows">executing a flow</a> you will be presented the Executing Flow page.  Alternatively, you can access these flows from the <a href="#flow-view">Flow View</a> page under the Executions tab, the History page, or the Executing page.</p>

<p>This page is similar to the Flow View page, except it shows status of running jobs. </p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executingflowpage.png" data-lightbox="executing-flow-page" title="Executing flow page">
      <img class="img-thumbnail img-figure" title="Executing Flow" src="./images/executingflowpage.png" ALT="Executing Flow Page" width="450" />
    </a>
  </div>
</div>

<p>Selecting the Job List will give a timeline of job executions. You can access the jobs and job logs directly from this list.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executingflowpagejobslist.png" data-lightbox="executing-flow-page-jobs-list" title="Executing flow page jobs list">
      <img class="img-thumbnail img-figure" title="Executing Flow" src="./images/executingflowpagejobslist.png" ALT="Executing Flow Page" width="450" />
    </a>
  </div>
</div>

<p>This page will auto update as long as the execution is not finished.</p>

<p>Some options that you are able to do on execution flows include the following:</p>

<ul>
	<li><strong>Cancel</strong> - kills all running jobs and fails the flow immediately. The flow state will be KILLED.</li>
	<li><strong>Pause</strong> - prevents new jobs from running. Currently running jobs proceed as usual.</li>
	<li><strong>Resume</strong> - resume a paused execution.</li>
	<li><strong>Retry Failed</strong> - only available when the flow is in a FAILED FINISHING state. Retry will restart all FAILED jobs while the flow is still active. Attempts will appear in the Jobs List page.</li>
	<li><strong>Prepare Execution</strong> - only available on a finished flow, regardless of success or failures. This will auto disable successfully completed jobs.</li>
</ul>

<h3>Executing Page</h3>

<p>Clicking on the Executing Tab in the header will show the Execution page. This page will show currently running executions as well as recently finished flows.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/executingflowspage.png" data-lightbox="executing-flows-page" title="Executions page">
      <img class="img-thumbnail img-figure" title="Executions" src="./images/executingflowspage.png" ALT="Executions" width="450" />
    </a>
  </div>
</div>

<h3>History Page</h3>

<p>Currently executing flows as well as completed executions will appear in the History page. Searching options are provided to find the execution you're looking for. Alternatively, you can view previous executions for a flow on the <a href="#flow-view">Flow View</a> execution tab.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/historypage.png" data-lightbox="history-page" title="Execution history">
      <img class="img-thumbnail img-figure" title="Executions" src="./images/historypage.png" ALT="Executions" width="450" />
    </a>
  </div>
</div>

            <hr>
						<h2 id="schedule-flow">Schedule Flow</h2>

<p>From the same panel that is used to <a href="#executing-flows">execute flows</a>, flows can be scheduled by clicking on the *Schedule* button.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/flexible-scheduling.png" data-lightbox="schedule-flow" title="Schedule flow">
      <img class="img-thumbnail" title="Schedule Flow" src="./images/flexible-scheduling.png" ALT="Schedule Flow" width="450" />
    </a>
  </div>
</div>

<p>Any flow options set will be preserved for the scheduled flow. For instance, if jobs are disabled, then the scheduled flow's jobs will also be disabled.<p>

<p>With new flexible scheduling feature in Azkaban 3.3, User are able to define a cron job following <a href="http://www.quartz-scheduler.org/documentation/quartz-2.x/tutorials/crontrigger.html">Quartz syntax</a>. One important change different from Quartz or cron is that Azkaban functions at the minute granularity at most. Therefore, second field in UI is labeled as a static "0". The <a href="https://github.com/azkaban/azkaban/wiki/New-Azkaban-Schedule-Introduction">Flexible Schedule Wiki</a> explains the details how to use. </p> 

<p>After scheduling, it should appear on the schedule page, where you can remove the scheduled job or set the SLA options.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/schedulepage.png" data-lightbox="schedule-flow" title="Schedule page">
      <img class="img-thumbnail" title="Schedule Page" src="./images/schedulepage.png" ALT="Schedule Page" width="450" />
    </a>
  </div>
</div>

<h3>SLA</h3>

<p>To add SLA notification or pre-emption, click on the SLA button. From here you can set the SLA alert emails. Rules can be added and applied to individual jobs or the flow itself. If duration threshold is exceeded, then an alert email can be set or the flow or job can be auto killed. If a job is killed due to missing the SLA, it will be retried based on the retry configuration of that job.
</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/slapanel.png" data-lightbox="schedule-flow" title="SLA panel">
      <img class="img-thumbnail" title="SLA Page" src="./images/slapanel.png" ALT="SLA Page" width="450" />
    </a>
  </div>
</div>

            <hr>
						<h2 id="job-page">Job Page</h2>

<p>Jobs make up individual tasks of a flow. To get to the jobs page, you can right click on a job in the Flow View, the Executing Flow view or the Project Page.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/jobpage.png" data-lightbox="job-page" title="Job page.">
      <img class="img-thumbnail img-figure" title="Jobs Page" src="./images/jobpage.png" ALT="Jobs Page" width="450" />
    </a>
  </div>
</div>

<p>From this page you can see the dependencies and dependents for a job as well as the global properties that the job will use.</p>

<h3>Job Edit</h3>

<p>Clicking on Job Edit will allow you to edit all the job properties except for certain reserved parameters, such as <code>type</code>, and <code>dependencies</code>. The changes to the parameters will affect an executing flow only if the job hasn't started to run yet. These overwrites of job properties will be overwritten by the next project upload.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/jobedit.png" data-lightbox="job-page" title="Job edit.">
      <img class="img-thumbnail img-figure" title="Jobs Page" src="./images/jobedit.png" ALT="Jobs Edit" width="450" />
    </a>
  </div>
</div>

<h3>Job History</h3>

<p.The Job history page will show a history of executions for a job in a flow as well as access to their logs. The logs have a retention policy and is by default purged after a month, although the execution information persists. The graph shows the runtimes for the jobs that appear on that page.</p>

<p>Any retries of a job will show as <code>executionid.attempt</code> number.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/jobhistorypage.png" data-lightbox="job-page" title="Job history page.">
      <img class="img-thumbnail img-figure" title="Job History" src="./images/jobhistorypage.png" ALT="Job History" width="450" />
    </a>
  </div>
</div>

            <hr>
						<h2 id="job-details">Job Details</h2>

<p>From an execution page, after clicking "Job List" and then "Details" for one of the jobs, you will arrive at the job details page. This page contains tabs for the "Job Logs" and a "Summary".</p>

<h3>Job Logs</h3>

<p>The job logs are stored in the database. They contain all the stdout and stderr output of the job.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/joblogs.png" data-lightbox="job-details" title="Job logs">
      <img class="img-thumbnail img-figure" src="./images/joblogs.png" alt="Job Logs" width="450" />
    </a>
  </div>
</div>

<h3>Job Summary</h3>

The <strong>Job Summary</strong> tab contains a summary of the information in the job logs. This includes:

<ul>
  <li><strong>Job Type</strong> - the jobtype of the job</li>
  <li><strong>Command Summary</strong> - the command that launched the job process, with fields such as the classpath and memory settings shown separately as well</li>
  <li><strong>Pig/Hive Job Summary</strong> - custom stats specific to Pig and Hive jobs</li>
  <li><strong>Map Reduce Jobs</strong> - a list of job ids of Map-Reduce jobs that were launched, linked to their job tracker pages</li>
</ul>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/jobsummary.png" data-lightbox="job-details" title="Job summary">
      <img class="img-thumbnail img-figure" src="./images/jobsummary.png" alt="Job Logs" width="450" />
    </a>
  </div>
</div>

            <hr>
						<h2 id="ajax-api">AJAX API</h2>

<p>
Often there's a desire to interact with Azkaban without having to use the web UI. Azkaban has some exposed ajax calls accessible through curl or some other HTTP request clients. All API calls require a proper authentication first.
<br/><br/>
Azkaban assumes the following request header in servlet's <code>isAjaxCall(HttpServletRequest request)</code> method:
<pre>
  Content-Type:     application/x-www-form-urlencoded
  X-Requested-With: XMLHttpRequest
</pre>
However, currently for most of APIs in this version, it is not checking the request header. Many APIs still treat a request as an ajax call if <code>request</code> simply contains the parameter <code>ajax</code>. Or even, several APIs is implicitly assuming it is an ajax call even without this keyword. For ease of use though, it is recommended to always keep the correct request header.
</p>

<h3 id="api-authenticate">Authenticate</h3>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> POST
  </li>
  <li>
    <strong>Request URL:</strong> /?action=login
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<p>This API helps authenticate a user and provides a <code>session.id</code> in response.</p>

<p>Once a <code>session.id</code> has been returned, until the session expires, this id can be used to do any API requests with a proper permission granted. A session expires if you log out, change machines, browsers or locations, if Azkaban is restarted, or if the session expires. The default session timeout is 24 hours (one day). You can re-login whether the session has expired or not. For the same user, a new session will always override old one.</p>

<p>
<strong> Importantly, </strong> <code>session.id</code> should be provided for almost all API calls (other than authentication). <code>session.id</code> can be simply appended as one of the request parameters, or set via the cookie: <code>azkaban.browser.session.id</code>. The two HTTP requests below are equivalent:
<pre class="code">
# a) Provide session.id parameter directly
curl -k --get --data "session.id=bca1d75d-6bae-4163-a5b0-378a7d7b5a91&ajax=fetchflowgraph&project=azkaban-test-project&flow=test" https://localhost:8443/manager

# b) Provide azkaban.browser.session.id cookie
curl -k --get -b "azkaban.browser.session.id=bca1d75d-6bae-4163-a5b0-378a7d7b5a91" --data "ajax=fetchflowgraph&project=azkaban-test-project&flow=test" https://localhost:8443/manager
</pre>
</p>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>action=login</td>
      <td>The fixed parameter indicating the login action.</td>
    </tr>
    <tr>
      <td>username</td>
      <td>The Azkaban username.</td>
    </tr>
    <tr>
      <td>password</td>
      <td>The corresponding password.</td>
    </tr>
  </tbody>
</table>


<h5><strong> Response Object </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>error</td>
      <td>Return an error message if the login attempt fails.</td>
    </tr>
    <tr>
      <td>session.id</td>
      <td>Return a session id if the login attempt succeeds.</td>
    </tr>
  </tbody>
</table>

A sample call via curl:

<pre class="code">
curl -k -X POST --data "action=login&amp;username=azkaban&amp;password=azkaban" https://localhost:8443
</pre>
A sample response:
<pre class="code">
{
  "status" : "success",
  "session.id" : "c001aba5-a90f-4daf-8f11-62330d034c0a"
}
</pre>

<h3 id="api-create-a-project">Create a Project</h3>
<p>
The ajax API for creating a new project.
<br>
<strong>Notice:</strong>
before uploading any project zip files, the project should be created first via this API.
</p>

<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> POST
  </li>
   <li>
    <strong>Request URL:</strong> /manager?action=create
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td><p> The user session id. </p></td>
    </tr>
    <tr>
      <td>action=create</td>
      <td>The fixed parameter indicating the create project action.</td>
    </tr>
    <tr>
      <td>name</td>
      <td>The project name to be uploaded.</td>
    </tr>
    <tr>
      <td>description</td>
      <td>The description for the project. This field cannot be empty.</td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object 1. (if the request succeeds): </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>status</td>
      <td>The status of the creation attempt.</td>
    </tr>
    <tr>
      <td>path</td>
      <td>The url path to redirect</td>
    </tr>
    <tr>
      <td>action</td>
      <td>The action that is suggested for the frontend to execute. (This is designed for the usage of the Azkaban frontend javascripts, external users can ignore this field.)</td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object 2. (if the request fails): </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>message</td>
      <td>The error message.</td>
    </tr>
    <tr>
      <td>error</td>
      <td>The error name.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k -X POST --data "session.id=9089beb2-576d-47e3-b040-86dbdc7f523e&name=aaaa&description=11" https://localhost:8443/manager?action=create
</pre>

A sample response:
<pre class="code">
{
  "status":"success",
  "path":"manager?project=aaaa",
  "action":"redirect"
}
</pre>

<h3 id="api-delete-a-project">Delete a Project</h3>
<p>
The ajax API for deleting an existing project. <br/>
<strong> Notice: </strong>
Currently no response message will be returned after finishing the delete operation.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /manager?delete=true
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td><p> The user session id. </p></td>
    </tr>
    <tr>
      <td>delete=true</td>
      <td>The fixed parameter to indicate the deleting project action.</td>
    </tr>
    <tr>
      <td>project</td>
      <td>The project name to be deleted.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>
<pre class="code">
curl -k --get --data "session.id=bca1d75d-6bae-4163-a5b0-378a7d7b5a91&delete=true&project=test-delete-project" https://localhost:8443/manager
</pre>


<h3 id="api-upload-a-project-zip">Upload a Project Zip</h3>
<p>
The ajax call to upload a project zip file. The zip file structure should follows the requirements described in
<a href="/docs/2.5/#upload-projects"> Upload Projects </a>.
<br/>
<strong>Notice: </strong> This API should be called after a project is successfully created.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> POST
  </li>
  <li>
    <strong>Content-Type:</strong> multipart/mixed
  </li>
   <li>
    <strong>Request URL:</strong> /manager?ajax=upload
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Body
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td><p> The user session id. </p></td>
    </tr>
    <tr>
      <td>ajax=upload</td>
      <td>The fixed parameter to the upload action.</td>
    </tr>
    <tr>
      <td>project</td>
      <td>The project name to be uploaded.</td>
    </tr>
    <tr>
      <td>file</td>
      <td>The project zip file. The type should be set as <code>application/zip</code> or <code>application/x-zip-compressed</code>.</td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>error</td>
      <td>The error message if the upload attempt fails.</td>
    </tr>
    <tr>
      <td>projectId</td>
      <td>The numerical id of the project</td>
    </tr>
    <tr>
      <td>version</td>
      <td>The version number of the upload</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k -i -H "Content-Type: multipart/mixed" -X POST --form 'session.id=e7a29776-5783-49d7-afa0-b0e688096b5e' --form 'ajax=upload' --form 'file=@myproject.zip;type=application/zip' --form 'project=MyProject;type/plain' https://localhost:8443/manager
</pre>

A response sample:
<pre class="code">
{
  "error" : "Installation Failed.\nError unzipping file.",
  "projectId" : "192",
  "version" : "1"
}
</pre>

<h3 id="api-fetch-flows-of-a-project">Fetch Flows of a Project</h3>
<p>
Given a project name, this API call fetches all flow ids of that project.
</p>

<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /manager?ajax=fetchprojectflows
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td><p> The user session id. </p></td>
    </tr>
    <tr>
      <td>ajax=fetchprojectflows</td>
      <td>The fixed parameter indicating the fetchProjectFlows action.</td>
    </tr>
    <tr>
      <td>project</td>
      <td>The project name to be fetched.</td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>project</td>
      <td>The project name.</td>
    </tr>
    <tr>
      <td>projectId</td>
      <td>The numerical id of the project.</td>
    </tr>
    <tr>
      <td>flows</td>
      <td>
        A list of flow ids.
        <p>
        <strong> Example values:</strong>
        [{"flowId": "aaa"}, {"flowId": "bbb"}]
      </td>
    </tr>
  </tbody>
</table>



<p>Here's a curl command sample:</p>
<pre class="code">
curl -k --get --data "session.id=6c96e7d8-4df5-470d-88fe-259392c09eea&ajax=fetchprojectflows&project=azkaban-test-project" https://localhost:8443/manager
</pre>

A response sample:
<pre class="code">
{
  "project" : "test-azkaban",
  "projectId" : 192,
  "flows" : [ {
    "flowId" : "test"
  }, {
    "flowId" : "test2"
  } ]
}
</pre>

<h3 id="api-fetch-jobs-of-a-flow">Fetch Jobs of a Flow</h3>
<p>
For a given project and a flow id, this API call fetches all the jobs that belong to this flow. It also returns the corresponding graph structure of those jobs.
</p>

<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /manager?ajax=fetchflowgraph
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
    <thead>
        <tr>
            <th>Parameter</th>
            <th> Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>session.id</td>
            <td><p> The user session id. </p></td>
        </tr>
        <tr>
            <td>ajax=fetchflowgraph</td>
            <td>The fixed parameter indicating the fetchProjectFlows action.</td>
        </tr>
        <tr>
            <td>project</td>
            <td>The project name to be fetched.</td>
        </tr>
        <tr>
            <td>flow</td>
            <td>The project id to be fetched.</td>
        </tr>
    </tbody>
</table>

<h5><strong> Response Object </strong></h5>
<table class="table table-striped table-condensed table-bordered">
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>project</td>
            <td>The project name.</td>
        </tr>
        <tr>
            <td>projectId</td>
            <td>The numerical id of the project.</td>
        </tr>
        <tr>
            <td>flow</td>
            <td>The flow id fetched.</td>
        </tr>
        <tr>
            <td>nodes</td>
            <td>
                A list of job nodes belonging to this flow.
                <p>
                  <strong> Structure:</strong>
                  <pre>
{
  "id": "job.id"
  "type": "job.type"
  "in": ["job.ids that this job is directly depending upon.
  Indirect ancestors is not included in this list"]
}
                  </pre>
                </p>
                <p>
                <strong> Example values:</strong>
                [{"id": "first_job", "type": "java"}, {"id": "second_job", "type": "command", "in":["first_job"]}]
              </p>
            </td>
        </tr>
    </tbody>
</table>

<p>Here's a curl command sample:</p>
<pre class="code">
curl -k --get --data "session.id=bca1d75d-6bae-4163-a5b0-378a7d7b5a91&ajax=fetchflowgraph&project=texter-1-1&flow=test" https://localhost:8445/manager
</pre>

A response sample:
<pre class="code">
{
  "project" : "azkaban-test-project",
  "nodes" : [ {
    "id" : "test-final",
    "type" : "command",
    "in" : [ "test-job-3" ]
  }, {
    "id" : "test-job-start",
    "type" : "java"
  }, {
    "id" : "test-job-3",
    "type" : "java",
    "in" : [ "test-job-2" ]
  }, {
    "id" : "test-job-2",
    "type" : "java",
    "in" : [ "test-job-start" ]
  } ],
  "flow" : "test",
  "projectId" : 192
}
</pre>

<h3 id="api-fetch-executions-of-a-flow">Fetch Executions of a Flow</h3>
Given a project name, and a certain flow, this API call provides a list of corresponding executions. Those executions are sorted in descendent submit time order. Also parameters are expected to specify the start index and the length of the list. This is originally used to handle pagination.

<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /manager?ajax=fetchFlowExecutions
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td><p> The user session id. </p></td>
    </tr>
    <tr>
      <td>ajax=fetchFlowExecutions</td>
      <td>The fixed parameter indicating the fetchFlowExecutions action.</td>
    </tr>
    <tr>
      <td>project</td>
      <td>The project name to be fetched.</td>
    </tr>
    <tr>
      <td>flow</td>
      <td>The flow id to be fetched.</td>
    </tr>
    <tr>
      <td>start</td>
      <td>The start index(inclusive) of the returned list.</td>
    </tr>
    <tr>
      <td>length</td>
      <td>The max length of the returned list. For example, if the start index is 2, and the length is 10, then the returned list will include executions of indices: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11].</td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>executions</td>
      <td>A list of execution objects, with the resquested start index and length.</td>
    </tr>
    <tr>
      <td>total</td>
      <td>The total number of all relevant execution</td>
    </tr>
    <tr>
      <td>project</td>
      <td>The project name fetched.</td>
    </tr>
    <tr>
      <td>projectId</td>
      <td>The numerical project id fetched.</td>
    </tr>
    <tr>
      <td>flow</td>
      <td>The flow id fetched.</td>
    </tr>
    <tr>
      <td>from</td>
      <td>The start index of the fetched executions</td>
    </tr>
    <tr>
      <td>length</td>
      <td>The length of the fetched executions.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>
<pre class="code">
curl -k --get --data "session.id=6c96e7d8-4df5-470d-88fe-259392c09eea&ajax=fetchFlowExecutions&project=azkaban-test-project&flow=test&start=0&length=3" https://localhost:8443/manager
</pre>

A response sample:
<pre class="code">
{
  "executions" : [ {
    "startTime" : 1407779928865,
    "submitUser" : "1",
    "status" : "FAILED",
    "submitTime" : 1407779928829,
    "execId" : 306,
    "projectId" : 192,
    "endTime" : 1407779950602,
    "flowId" : "test"
  }, {
    "startTime" : 1407779877807,
    "submitUser" : "1",
    "status" : "FAILED",
    "submitTime" : 1407779877779,
    "execId" : 305,
    "projectId" : 192,
    "endTime" : 1407779899599,
    "flowId" : "test"
  }, {
    "startTime" : 1407779473354,
    "submitUser" : "1",
    "status" : "FAILED",
    "submitTime" : 1407779473318,
    "execId" : 304,
    "projectId" : 192,
    "endTime" : 1407779495093,
    "flowId" : "test"
  } ],
  "total" : 16,
  "project" : "azkaban-test-project",
  "length" : 3,
  "from" : 0,
  "flow" : "test",
  "projectId" : 192
}
</pre>

<h3 id="api-fetch-running-executions-of-a-flow"> Fetch Running Executions of a Flow </h3>
<p>
Given a project name and a flow id, this API call fetches only executions that are currently running.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /executor?ajax=getRunning
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
    <thead>
        <tr>
            <th>Parameter</th>
            <th> Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>session.id</td>
            <td><p> The user session id. </p></td>
        </tr>
        <tr>
            <td>ajax=getRunning</td>
            <td>The fixed parameter indicating the getRunning action.</td>
        </tr>
        <tr>
            <td>project</td>
            <td>The project name to be fetched.</td>
        </tr>
         <tr>
            <td>flow</td>
            <td>The flow id to be fetched.</td>
        </tr>
    </tbody>
</table>

<h5><strong> Response Object </strong></h5>
<table class="table table-striped table-condensed table-bordered">
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>execIds</td>
            <td>A list of execution ids fetched.
              <p>
                <strong> Example values: </strong>
                [301, 302, 111, 999]
              </p>
            </td>
        </tr>
    </tbody>
</table>

<p>Here's a curl command sample:</p>
<pre class="code">
curl -k --data "session.id=34ba08fd-5cfa-4b65-94c4-9117aee48dda&ajax=getRunning&project=azkaban-test-project&flow=test" https://localhost:8443/executor
</pre>

A response sample:
<pre class="code">
{
  "execIds": [301, 302]
}
</pre>

<h3 id="api-execute-a-flow">Execute a Flow</h3>
<p>
This API executes a flow via an ajax call, supporting a rich selection of different options. Running an individual job can also be achieved via this API
by disabling all other jobs in the same flow.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
  <li>
    <strong> Request URL: </strong> /executor?ajax=executeFlow
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>
        <p> The user session id. </p>
        <p>
          <strong>Example Values:</strong> 30d538e2-4794-4e7e-8a35-25a9e2fd5300
        </p>
      </td>
    </tr>
    <tr>
      <td> ajax=executeFlow </td>
      <td>
        <p>
        The fixed parameter indicating the current ajax action is executeFlow.
        </p>
      </td>
    </tr>
    <tr>
      <td>project</td>
      <td>
        <p>The project name of the executing flow.</p>
        <p> <strong> Example Values: </strong> run-all-jobs </p>
      </td>
    </tr>
    <tr>
      <td>flow</td>
      <td>
        <p>The flow id to be executed.</p>
        <p> <strong> Example Values: </strong> test-flow </p>
      </td>
    </tr>
    <tr>
      <td>disabled (optional)</td>
      <td>
        <p>A list of job names that should be disabled for this execution. Should be formatted as a JSON Array String.</p>
        <p> <strong> Example Values: </strong> ["job_name_1", "job_name_2", "job_name_N"] </p>
      </td>
    </tr>
    <tr>
      <td>successEmails (optional)</td>
      <td>
        <p>A list of emails to be notified if the execution succeeds. All emails are delimitted with [,|;|\\s+].</p>
        <p> <strong> Example Values: </strong> foo@email.com,bar@email.com </p>
      </td>
    </tr>
    <tr>
      <td>failureEmails (optional)</td>
      <td>
        <p>A list of emails to be notified if the execution fails. All emails are delimitted with [,|;|\\s+].</p>
        <p> <strong> Example Values: </strong> foo@email.com,bar@email.com </p>
      </td>
    </tr>
    <tr>
      <td>successEmailsOverride (optional)</td>
      <td>
        <p>Whether uses system default email settings to override successEmails.</p>
        <p> <strong> Possible Values: </strong> true, false </p>
      </td>
    </tr>
    <tr>
      <td>failureEmailsOverride (optional)</td>
      <td>
        <p>Whether uses system default email settings to override failureEmails.</p>
        <p> <strong> Possible Values: </strong> true, false </p>
      </td>
    </tr>
    <tr>
      <td>notifyFailureFirst (optional)</td>
      <td>
        <p>Whether sends out email notifications as long as the first failure occurs.</p>
        <p> <strong> Possible Values: </strong> true, false </p>
      </td>
    </tr>
    <tr>
      <td>notifyFailureLast (optional)</td>
      <td>
        <p>Whether sends out email notifications as long as the last failure occurs.</p>
        <p> <strong> Possible Values: </strong> true, false </p>
      </td>
    </tr>
    <tr>
      <td>failureAction (Optional)</td>
      <td>
        <p>If a failure occurs, how should the execution behaves.</p>
        <p> <strong> Possible Values: </strong> finishCurrent, cancelImmediately, finishPossible </p>
      </td>
    </tr>
    <tr>
      <td>concurrentOption (Optional)</td>
      <td>
        <p>Concurrent choices. Use ignore if nothing specifical is required. </p>
        <p> <strong> Possible Values: </strong> ignore, pipeline, skip</p>
      </td>
    </tr>
    <tr>
      <td>flowOverride[flowProperty] (Optional)</td>
      <td>
        <p>Override specified flow property with specified value. </p>
        <p> <strong> Example Values : </strong> flowOverride[failure.email]=test@gmail.com </p>
      </td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object </strong></h5>
<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>error</td>
      <td>Error message if the call has failed</td>
    </tr>
    <tr>
      <td>flow</td>
      <td>The executed flow id</td>
    </tr>
    <tr>
      <td>execid</td>
      <td>The execution id</td>
    </tr>
  </tbody>
</table>
Here is a curl command example:
<pre class="code">
curl -k --get --data 'session.id=189b956b-f39f-421e-9a95-e3117e7543c9' --data 'ajax=executeFlow' --data 'project=azkaban-test-project' --data 'flow=test' https://localhost:8443/executor
</pre>

Sample response:
<pre class="code">
{
  message: "Execution submitted successfully with exec id 295",
  project: "foo-demo",
  flow: "test",
  execid: 295
}
</pre>

<h3 id="api-cancel-a-flow-execution"> Cancel a Flow Execution </h3>

<p>
Given an execution id, this API call cancels a running flow. If the flow is not running, it will return an error message.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
  <li>
    <strong> Request URL: </strong> /executor?ajax=cancelFlow
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>
        <p> The user session id. </p>
      </td>
    </tr>
    <tr>
      <td> ajax=cancelFlow </td>
      <td>
        <p>
        The fixed parameter indicating the current ajax action is cancelFlow.
        </p>
      </td>
    </tr>
    <tr>
      <td>execid</td>
      <td>
        <p> The execution id. </p>
      </td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k --data "session.id=34ba08fd-5cfa-4b65-94c4-9117aee48dda&ajax=cancelFlow&execid=302" https://localhost:8443/executor
</pre>

A response sample if succeeds:
<pre class="code">
{ }
</pre>
A response sample if fails:
<pre class="code">
{
  "error" : "Execution 302 of flow test isn't running."
}
</pre>


<h3 id="api-schedule-a-flow">Schedule a period-based Flow (Deprecated)</h3>

<p>
This API call schedules a period-based flow.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> POST
  </li>
  <li>
    <strong>Request URL:</strong> /schedule?ajax=scheduleFlow
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong>Request Parameters</strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>The user session id.</td>
    </tr>
    <tr>
      <td>ajax=scheduleFlow</td>
      <td>The fixed parameter indicating the action is to schedule a flow.</td>
    </tr>
    <tr>
      <td>projectName</td>
      <td>The name of the project.</td>
    </tr>
    <tr>
      <td>projectId</td>
      <td>The id of the project. You can find this with <a href="#api-fetch-flows-of-a-project">Fetch Flows of a Project</a>.</td>
    </tr>
    <tr>
      <td>flowName</td>
      <td>The name of the flow.</td>
    </tr>
    <tr>
      <td>scheduleTime(with timezone)</td>
      <td>The time to schedule the flow. Example: 12,00,pm,PDT (Unless UTC is specified, Azkaban will take current server's default timezone instead)</td>
    </tr>
    <tr>
      <td>scheduleDate</td>
      <td>The date to schedule the flow. Example: 07/22/2014</td>
    </tr>
    <tr>
      <td>is_recurring=on (optional)</td>
      <td>Flags the schedule as a recurring schedule.</td>
    </tr>
    <tr>
      <td>period  (optional)</td>
      <td>Specifies the recursion period. Depends on the "is_recurring" flag being set. Example: 5w
        <p> <strong> Possible Values: </strong>
        <table>
          <tbody>
            <tr>
              <td>M</td>
              <td>Months</td>
            </tr>
            <tr>
              <td>w</td>
              <td>Weeks</td>
            </tr>
            <tr>
              <td>d</td>
              <td>Days</td>
            </tr>
            <tr>
              <td>h</td>
              <td>Hours</td>
            </tr>
            <tr>
              <td>m</td>
              <td>Minutes</td>
            </tr>
            <tr>
              <td>s</td>
              <td>Seconds</td>
            </tr>
          </tbody>
        </table>
      </td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">

  # a) One time schedule
  curl -k https://HOST:PORT/schedule -d "ajax=scheduleFlow&projectName=PROJECT_NAME&flow=FLOW_NAME&projectId=PROJECT_ID&scheduleTime=12,00,pm,PDT&scheduleDate=07/22/2014" -b azkaban.browser.session.id=SESSION_ID

  # b) Recurring schedule
  curl -k https://HOST:PORT/schedule -d "ajax=scheduleFlow&is_recurring=on&period=5w&projectName=PROJECT_NAME&flow=FLOW_NAME&projectId=PROJECT_ID&scheduleTime=12,00,pm,PDT&scheduleDate=07/22/2014" -b azkaban.browser.session.id=SESSION_ID

</pre>

An example success response:
<pre class="code">
{
  "message" : "PROJECT_NAME.FLOW_NAME scheduled.",
  "status" : "success"
}
</pre>
An example failure response:
<pre class="code">
{
  "message" : "Permission denied. Cannot execute FLOW_NAME",
  "status" : "error"
}
</pre>
An example failure response for invalid schedule period:
<pre class="code">
{
  "message" : "PROJECT_NAME.FLOW_NAME scheduled.",
  "error" : "Invalid schedule period unit 'A",
  "status" : "success"
}
</pre>

<h3 id="api-flexible-schedule">Flexible scheduling using Cron</h3>

<p>
This API call schedules a flow by a cron Expression. Cron is a UNIX tool that has been widely used for a long time, and we use <a href="http://www.quartz-scheduler.org/">Quartz library</a> to parse cron Expression. All cron schedules follow the timezone defined in azkaban web server (the timezone ID is obtained by <i>java.util.TimeZone.getDefault().getID()</i>).
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> POST
  </li>
  <li>
    <strong>Request URL:</strong> /schedule?ajax=scheduleCronFlow
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong>Request Parameters</strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>The user session id.</td>
    </tr>
    <tr>
      <td>ajax=scheduleCronFlow</td>
      <td>The fixed parameter indicating the action is to use cron to schedule a flow.</td>
    </tr>
    <tr>
      <td>projectName</td>
      <td>The name of the project.</td>
    </tr>
    <tr>
      <td>flow</td>
      <td>The name of the flow.</td>
    </tr>
    <tr>
      <td>cronExpression</td>
      <td>A CRON expression is a string comprising 6 or 7 fields separated by white space that represents a set of times. In azkaban, we use <a href="http://www.quartz-scheduler.org/documentation/quartz-2.x/tutorials/crontrigger.html"> Quartz Cron Format</a>.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k -d ajax=scheduleCronFlow -d projectName=wtwt -d flow=azkaban-training --data-urlencode cronExpression="0 23/30 5,7-10 ? * 6#3" -b "azkaban.browser.session.id=XXXXXXXXXXXXXX" http://localhost:8081/schedule
</pre>

An example success response:
<pre class="code">
{
  "message" : "PROJECT_NAME.FLOW_NAME scheduled.",
  "scheduleId" : SCHEDULE_ID,
  "status" : "success"
}
</pre>
An example failure response:
<pre class="code">
{
  "message" : "Cron expression must exist.",
  "status" : "error"
}
</pre>
<pre class="code">
{
  "message" : "Permission denied. Cannot execute FLOW_NAME",
  "status" : "error"
}
</pre>
An example failure response for invalid cron expression:
<pre class="code">
{
  "message" : "This expression <*****> can not be parsed to quartz cron.",
  "status" : "error"
}
</pre>

<h3 id="api-fetch-schedule">Fetch a Schedule</h3>

<p>
Given a project id and a flow id, this API call fetches the schedule.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
  <li>
    <strong>Request URL:</strong> /schedule?ajax=fetchSchedule
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong>Request Parameters</strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>The user session id.</td>
    </tr>
    <tr>
      <td>ajax=fetchSchedule</td>
      <td>The fixed parameter indicating the schedule.</td>
    </tr>
    <tr>
      <td>projectId</td>
      <td>The id of the project.</td>
    </tr>
    <tr>
      <td>flowId</td>
      <td>The name of the flow.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k --get --data "session.id=XXXXXXXXXXXXXX&ajax=fetchSchedule&projectId=1&flowId=test" http://localhost:8081/schedule
</pre>

An example success response:
<pre class="code">
{
  "schedule" : {
    "cronExpression" : "0 * 9 ? * *",
    "nextExecTime" : "2017-04-01 09:00:00",
    "period" : "null",
    "submitUser" : "azkaban",
    "executionOptions" : {
      "notifyOnFirstFailure" : false,
      "notifyOnLastFailure" : false,
      "failureEmails" : [ ],
      "successEmails" : [ ],
      "pipelineLevel" : null,
      "queueLevel" : 0,
      "concurrentOption" : "skip",
      "mailCreator" : "default",
      "memoryCheck" : true,
      "flowParameters" : {
      },
      "failureAction" : "FINISH_CURRENTLY_RUNNING",
      "failureEmailsOverridden" : false,
      "successEmailsOverridden" : false,
      "pipelineExecutionId" : null,
      "disabledJobs" : [ ]
    },
    "scheduleId" : "3",
    "firstSchedTime" : "2017-03-31 11:45:21"
  }
}
</pre>
If there is no schedule, empty response returns.
<pre class="code">
{}
</pre>

<h3 id="api-unschedule-a-flow">Unschedule a Flow</h3>

<p>
This API call unschedules a flow.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> POST
  </li>
  <li>
    <strong>Request URL:</strong> /schedule?action=removeSched
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong>Request Parameters</strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>The user session id.</td>
    </tr>
    <tr>
      <td>action=removeSched</td>
      <td>The fixed parameter indicating the action is to unschedule a flow.</td>
    </tr>
    <tr>
      <td>scheduleId</td>
      <td>The id of the schedule. You can find this in the Azkaban UI on the /schedule page.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k https://HOST:PORT/schedule -d "action=removeSched&scheduleId=SCHEDULE_ID" -b azkaban.browser.session.id=SESSION_ID
</pre>

An example success response:
<pre class="code">
{
  "message" : "flow FLOW_NAME removed from Schedules.",
  "status" : "success"
}
</pre>
An example failure response:
<pre class="code">
{
  "message" : "Schedule with ID SCHEDULE_ID does not exist",
  "status" : "error"
}
</pre>

<h3 id="api-set-sla">Set a SLA</h3>

<p>
This API call sets a SLA.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> POST
  </li>
  <li>
    <strong>Request URL:</strong> /schedule?ajax=setSla
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong>Request Parameters</strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>The user session id.</td>
    </tr>
    <tr>
      <td>ajax=setSla</td>
      <td>The fixed parameter indicating the action is to set a SLA.</td>
    </tr>
    <tr>
      <td>scheduleId</td>
      <td>The id of the shchedule. You can find this with <a href="#api-fetch-schedule">Fetch a Schedule</a>.</td>
    </tr>
    <tr>
      <td>slaEmails</td>
      <td>A list of SLA alert emails.
        <p>
        <strong> Example:</strong>
        slaEmails=a@example.com;b@example.com
      </td>
    </tr>
    <tr>
      <td>settings[...]</td>
      <td>Rules of SLA. Format is settings[...]=[id],[rule],[duration],[emailAction],[killAction].
        <p>
        <strong> Example:</strong>
        settings[0]=aaa,SUCCESS,5:00,true,false
      </td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k -d "ajax=setSla&scheduleId=1&slaEmails=a@example.com;b@example.com&settings[0]=aaa,SUCCESS,5:00,true,false&settings[1]=bbb,SUCCESS,10:00,false,true" -b "azkaban.browser.session.id=XXXXXXXXXXXXXX" "http://localhost:8081/schedule"
</pre>

An example success response:
<pre class="code">
{}
</pre>
An example failure response:
<pre class="code">
{
  "error" : "azkaban.scheduler.ScheduleManagerException: Unable to parse duration for a SLA that needs to take actions!"
}
</pre>

<h3 id="api-fetch-sla">Fetch a SLA</h3>

<p>
Given a schedule id, this API call fetches the SLA.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
  <li>
    <strong>Request URL:</strong> /schedule?ajax=slaInfo
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong>Request Parameters</strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>The user session id.</td>
    </tr>
    <tr>
      <td>ajax=slaInfo</td>
      <td>The fixed parameter indicating the SLA.</td>
    </tr>
    <tr>
      <td>scheduleId</td>
      <td>The id of the shchedule. You can find this with <a href="#api-fetch-schedule">Fetch a Schedule</a>.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k --get --data "session.id=XXXXXXXXXXXXXX&ajax=slaInfo&scheduleId=1" http://localhost:8081/schedule"
</pre>

An example success response:
<pre class="code">
{
  "settings" : [ {
    "duration" : "300m",
    "rule" : "SUCCESS",
    "id" : "aaa",
    "actions" : [ "EMAIL" ]
  }, {
    "duration" : "600m",
    "rule" : "SUCCESS",
    "id" : "bbb",
    "actions" : [ "KILL" ]
  } ],
  "slaEmails" : [ "a@example.com", "b@example.com" ],
  "allJobNames" : [ "aaa", "ccc", "bbb", "start", "end" ]
}
</pre>

<h3 id="api-pause-a-flow-execution"> Pause a Flow Execution</h3>
<p>
Given an execution id, this API pauses a running flow. If an execution has already been paused, it will not return any error; if an execution is not running, it will return an error message.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
  <li>
    <strong> Request URL: </strong> /executor?ajax=pauseFlow
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>
        <p> The user session id. </p>
      </td>
    </tr>
    <tr>
      <td> ajax=pauseFlow </td>
      <td>
        <p>
        The fixed parameter indicating the current ajax action is pauseFlow.
        </p>
      </td>
    </tr>
    <tr>
      <td>execid</td>
      <td>
        <p> The execution id. </p>
      </td>
    </tr>
  </tbody>
</table>
<p>Here's a curl command sample:</p>

<pre class="code">
curl -k --data "session.id=34ba08fd-5cfa-4b65-94c4-9117aee48dda&ajax=pauseFlow&execid=303" https://localhost:8443/executor
</pre>

A response sample (if succeeds, or pauseFlow is called multiple times):
<pre class="code">
{ }
</pre>
A response sample (if fails, only when the flow is not actually running):
<pre class="code">
{
  "error" : "Execution 303 of flow test isn't running."
}
</pre>

<h3 id="api-resume-a-flow-execution"> Resume a Flow Execution</h3>
<p>
Given an execution id, this API resumes a paused running flow. If an execution has already been resumed, it will not return any errors; if an execution is not runnning, it will return an error message.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
  <li>
    <strong> Request URL: </strong> /executor?ajax=resumeFlow
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td>
        <p> The user session id. </p>
      </td>
    </tr>
    <tr>
      <td> ajax=resumeFlow </td>
      <td>
        <p>
        The fixed parameter indicating the current ajax action is resumeFlow.
        </p>
      </td>
    </tr>
    <tr>
      <td>execid</td>
      <td>
        <p> The execution id. </p>
      </td>
    </tr>
  </tbody>
</table>
<p>Here's a curl command sample:</p>

<pre class="code">
curl -k --data "session.id=34ba08fd-5cfa-4b65-94c4-9117aee48dda&ajax=resumeFlow&execid=303" https://localhost:8443/executor
</pre>

A response sample (if succeeds, or resumeFlow is called multiple times):
<pre class="code">
{ }
</pre>
A response sample (if fails, only when the flow is not actually running):
<pre class="code">
{
  "error" : "Execution 303 of flow test isn't running."
}
</pre>

<h3 id="api-fetch-a-flow-execution"> Fetch a Flow Execution</h3>
<p>
Given an execution id, this API call fetches all the detailed information of that execution, including a list of all the job executions.
</p>

<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /executor?ajax=fetchexecflow
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>
<table class="table table-striped table-condensed table-bordered">
    <thead>
        <tr>
            <th>Parameter</th>
            <th> Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>session.id</td>
            <td><p> The user session id. </p></td>
        </tr>
        <tr>
            <td>ajax=fetchexecflow</td>
            <td>The fixed parameter indicating the fetchexecflow action.</td>
        </tr>
        <tr>
            <td>execid</td>
            <td>The execution id to be fetched.</td>
        </tr>
    </tbody>
</table>

<h5><strong> Response Object </strong></h5>
<p>
  It returns a detailed information about the execution (check the example below). One thing to
  notice is that the field <code>nodes[i].in</code> actually indicates what are the dependencies of this node.
</p>

<p>Here's a curl command sample:</p>
<pre class="code">
curl -k --data "session.id=34ba08fd-5cfa-4b65-94c4-9117aee48dda&ajax=fetchexecflow&execid=304" https://localhost:8443/executor
</pre>

A response sample:
<pre class="code">
{
  "attempt" : 0,
  "submitUser" : "1",
  "updateTime" : 1407779495095,
  "status" : "FAILED",
  "submitTime" : 1407779473318,
  "projectId" : 192,
  "flow" : "test",
  "endTime" : 1407779495093,
  "type" : null,
  "nestedId" : "test",
  "startTime" : 1407779473354,
  "id" : "test",
  "project" : "test-azkaban",
  "nodes" : [ {
    "attempt" : 0,
    "startTime" : 1407779495077,
    "id" : "test",
    "updateTime" : 1407779495077,
    "status" : "CANCELLED",
    "nestedId" : "test",
    "type" : "command",
    "endTime" : 1407779495077,
    "in" : [ "test-foo" ]
  }, {
    "attempt" : 0,
    "startTime" : 1407779473357,
    "id" : "test-bar",
    "updateTime" : 1407779484241,
    "status" : "SUCCEEDED",
    "nestedId" : "test-bar",
    "type" : "pig",
    "endTime" : 1407779484236
  }, {
    "attempt" : 0,
    "startTime" : 1407779484240,
    "id" : "test-foobar",
    "updateTime" : 1407779495073,
    "status" : "FAILED",
    "nestedId" : "test-foobar",
    "type" : "java",
    "endTime" : 1407779495068,
    "in" : [ "test-bar" ]
  }, {
    "attempt" : 0,
    "startTime" : 1407779495069,
    "id" : "test-foo",
    "updateTime" : 1407779495069,
    "status" : "CANCELLED",
    "nestedId" : "test-foo",
    "type" : "java",
    "endTime" : 1407779495069,
    "in" : [ "test-foobar" ]
  } ],
  "flowId" : "test",
  "execid" : 304
}
</pre>


<h3 id="api-fetch-execution-job-logs"> Fetch Execution Job Logs </h3>
<p>
Given an execution id and a job id, this API call fetches the correponding job logs. The log text can be quite large sometimes, so this API call also expects the parameters <code> offset</code> and <code>length</code> to be specified.
</p>

<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /executor?ajax=fetchExecJobLogs
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td><p> The user session id. </p></td>
    </tr>
    <tr>
      <td>ajax=fetchExecJobLogs</td>
      <td>The fixed parameter indicating the fetchExecJobLogs action.</td>
    </tr>
    <tr>
      <td>execid</td>
      <td>The unique id for an execution.</td>
    </tr>
    <tr>
      <td>jobId</td>
      <td>The unique id for the job to be fetched.</td>
    </tr>
    <tr>
      <td>offset</td>
      <td>The offset for the log data.</td>
    </tr>
    <tr>
      <td>length</td>
      <td>The length of the log data. For example, if the offset set is 10 and the length is 1000, the returned log will starts from the 10th character and has a length of 1000 (less if the remaining log is less than 1000 long).</td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>data</td>
      <td>The text data of the logs.</td>
    </tr>
    <tr>
      <td>offset</td>
      <td>The offset for the log data.</td>
    </tr>
    <tr>
      <td>length</td>
      <td>The length of the log data.</td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k --data "session.id=9089beb2-576d-47e3-b040-86dbdc7f523e&ajax=fetchExecJobLogs&execid=297&jobId=test-foobar&offset=0&length=100" https://localhost:8443/executor
</pre>

A response sample:
<pre class="code">
{
  "data" : "05-08-2014 16:53:02 PDT test-foobar INFO - Starting job test-foobar at 140728278",
  "length" : 100,
  "offset" : 0
}
</pre>

<h3 id="api-fetch-flow-execution-updates"> Fetch Flow Execution Updates </h3>
<p>
This API call fetches the updated information for an execution. It filters by <code>lastUpdateTime</code> which only returns job information updated afterwards.
</p>
<ul class="list-unstyled">
  <li>
    <strong>Method:</strong> GET
  </li>
   <li>
    <strong>Request URL:</strong> /executor?ajax=fetchexecflowupdate
  </li>
  <li>
    <strong>Parameter Location:</strong> Request Query String
  </li>
</ul>

<h5><strong> Request Parameters </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>session.id</td>
      <td><p> The user session id. </p></td>
    </tr>
    <tr>
      <td>ajax=fetchexecflowupdate</td>
      <td>The fixed parameter indicating the fetch execution updates action.</td>
    </tr>
    <tr>
      <td>execid</td>
      <td>The execution id.</td>
    </tr>
    <tr>
      <td>lastUpdateTime</td>
      <td>The criteria to filter by last update time. Set the value to be <code>-1</code> if all job information are needed.</td>
    </tr>
  </tbody>
</table>

<h5><strong> Response Object </strong></h5>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>id</td>
      <td>The flow id.</td>
    </tr>
    <tr>
      <td>flow</td>
      <td>The flow name.</td>
    </tr>
    <tr>
      <td>startTime</td>
      <td>The start time of this flow execution.</td>
    </tr>
    <tr>
      <td>updateTime</td>
      <td>The last updated time of this flow execution.</td>
    </tr>
    <tr>
      <td>endTime</td>
      <td>The end time of this flow execution (if it finishes).</td>
    </tr>
    <tr>
      <td>status</td>
      <td>The current status of the flow.</td>
    </tr>
    <tr>
      <td>attempt</td>
      <td>The attempt number of this flow execution.</td>
    </tr>
    <tr>
      <td>nodes</td>
      <td>Information for each execution job. Containing the following fields:
        <pre class="code">
{
  "attempt": String,
  "startTime": Number,
  "id": String (the job id),
  "updateTime":Number,
  "status": String,
  "endTime": Number
}
        </pre>
      </td>
    </tr>
  </tbody>
</table>

<p>Here's a curl command sample:</p>

<pre class="code">
curl -k --data "execid=301&lastUpdateTime=-1&session.id=6668c180-efe7-46a-8dd2-e36508b440d8" https://localhost:8443/executor?ajax=fetchexecflowupdate
</pre>

A response sample:
<pre class="code">
{
  "id" : "test",
  "startTime" : 1407778382894,
  "attempt" : 0,
  "status" : "FAILED",
  "updateTime" : 1407778404708,
  "nodes" : [ {
    "attempt" : 0,
    "startTime" : 1407778404683,
    "id" : "test",
    "updateTime" : 1407778404683,
    "status" : "CANCELLED",
    "endTime" : 1407778404683
  }, {
    "attempt" : 0,
    "startTime" : 1407778382913,
    "id" : "test-job-1",
    "updateTime" : 1407778393850,
    "status" : "SUCCEEDED",
    "endTime" : 1407778393845
  }, {
    "attempt" : 0,
    "startTime" : 1407778393849,
    "id" : "test-job-2",
    "updateTime" : 1407778404679,
    "status" : "FAILED",
    "endTime" : 1407778404675
  }, {
    "attempt" : 0,
    "startTime" : 1407778404675,
    "id" : "test-job-3",
    "updateTime" : 1407778404675,
    "status" : "CANCELLED",
    "endTime" : 1407778404675
  } ],
  "flow" : "test",
  "endTime" : 1407778404705
}
</pre>

					</div>
          <div class="docs-section">
		
<h2 id="how-to">How Tos</h2>

<h4>Force execution to an executor</h4>
<p>Only users with admin privileges can use this override. In flow params: set <code>"useExecutor" = EXECUTOR_ID </code>.</p>


<h4>Setting flow priority in multiple executor mode</h4>
<p>Only users with admin privileges can use this property. In flow params: set <code>"flowPriority" = PRIORITY </code>. Higher numbers get executed first.</p>

<h4>Enabling and Disabling Queue in multiple executor mode</h4>
<p>Only users with admin privileges can use this action. Use curl or simply visit following URL:- 
	<ul>
		<li>Enable: <code>WEBSERVER_URL/executor?ajax=disableQueueProcessor</code></li>
		<li>Disable: <code>WEBSERVER_URL/executor?ajax=enableQueueProcessor</code></li>
	</ul>
</p>

<h4>Reloading executors in multiple executor mode</h4>
<p>Only users with admin privileges can use this action. This action need at least one active executor to be successful. Use curl or simply visit following URL:-
	<code>WEBSERVER_URL/executor?ajax=reloadExecutors</code>
</p>	

<h4>Logging job logs to a Kafka cluster</h4>
<p>Azkaban supports sending job logs to a log ingestion (such as ELK) cluster via a Kafka appender. In order to enable this in Azkaban, you will need to set two exec server properties (shown here with sample values):
</p>

<pre class="code">
azkaban.server.logging.kafka.brokerList=localhost:9092
azkaban.server.logging.kafka.topic=azkaban-logging
</pre>

<p>These configure where Azkaban can find your Kafka cluster, and also which topic to put the logs under. Failure to provide these parameters will result in
Azkaban refusing to create a Kafka appender upon requesting one.</p>
<p>In order to configure a job to send its logs to Kafka, the following job property needs to be set to true:
</p>

<pre class="code">
azkaban.job.logging.kafka.enable=true
</pre>

<p>Jobs with this setting enabled will broadcast its log messages in JSON form to the Kafka cluster. It has the following structure:
</p>

<pre class="code">
{
  "projectname": "Project name",
  "level": "INFO or ERROR",
  "submituser": "Someone",
  "projectversion": "Project version",
  "category": "Class name",
  "message": "Some log message",
  "logsource": "userJob",
  "flowid": "ID of flow",
  "execid": "ID of execution"
}
</pre>

          </div>
          <div class="docs-section">
						<div class="page-header">
  <h1 id="plugins">Plugins</h1>
</div>

<p>Azkaban is designed to be modular. We are able to plug in code to add viewer pages or execute jobs in a customizable manner. These pages will describe the azkaban-plugins that can be downloaded from <a href="http://azkaban.github.io/downloads.html">the download page</a> and how to extend Azkaban by creating your own plugins or extending an existing one.</p>

            <hr>
						
<h2 id="hadoopsecuritymanager">Support for Hadoop Security</h2>

<p>The most common adoption of Azkaban has been in the big data platforms such as Hadoop, etc. Azkaban's jobtype plugin system allows most flexible support to such systems. </p>

<p>Azkaban is able to support all Hadoop versions, with support for Hadoop security features; Azkaban is able to support various ecosystem components with all different versions, such as different versions of pig, hive, on the same instance.</p>

<p>A common pattern to achieve this is by using the <code>HadoopSecurityManager</code> class, which handles talking to a Hadoop cluster and take care of Hadoop security, in a secure way.</p>

<h3>Hadoop Security with Kerberos, Hadoop Tokens</h3>

<p>When Hadoop is used in enterprise production environment, it is advisable to have its security feature turned on, to protect your data and guard against mistakes.</p>

<h4>Kerberos Authentication</h4>

<p>The most common authentication provided by Apache Hadoop is via Kerberos, which requires a KDC to authenticate users and services.</p>

<p>A user can authenticate with KDC via username/password or use a keytab. KDC distributes a tgt to authenticated users. Hadoop services, such as name node and job tracker, can use this tgt to verify this is authenticated user.</p>

<h4>Hadoop Tokens</h4>

<p>Once a user is authenticated with Hadoop services, Hadoop will issue tokens to the user so that its internal services won't flood KDC. For a description of tokens, see <a href="http://hortonworks.com/blog/the-role-of-delegation-tokens-in-apache-hadoop-security/" target="_blank">here</a>.</p>

<h4>Hadoop SecurityManager</h4>

<p>For human users, one authenticate with KDC with a kinit command. But for scheduler such as Azkaban that runs jobs on behalf as other users, it needs to acquire tokens that will be used by the users. Specific Azkaban job types should handle this, with the use of <code>HadoopSecurityManager</code> class.</p>

<p>For instance, when Azkaban loads the pig job type, it will initiate a HadoopSecurityManager that is authenticated with the desired KDC and Hadoop Cluster. The pig job type conf should specify which tokens are needed to talk to different services. At minimum it needs tokens from name node and job tracker. When a pig job starts, it will go to the HadoopSecurityManager to acquire all those tokens. When the user process finishes, the pig job type calls HadoopSecurityManager again to cancel all those tokens.</p>

<h3>Settings Common to All Hadoop Clusters</h3>

<p>When a user program wants to talk to a Hadoop cluster, it needs to know where are the name node and job tracker. It also needs to know how to authenticate with them. These information are all in the Hadoop config files that are normally in <code>$HADOOP_HOME/conf</code>. For this reason, this conf directory as well as the hadoop-core jar need to be on azkaban executor server classpath.</p>

<p>If you are using Hive that uses HCat as its metastore, you also need relevant hive jars and hive conf on the classpath as well.</p>

<h4>Native Library</h4>

<p>Most likely your Hadoop platform depends on some native library, this should be specified in java.library.path in azkaban executor server.</p>

<h4>temp dir</h4>

<p>Besides those, many tools on Hadoop, such as Pig/Hive/Crunch write files into temporary directory. By default, they all go to <code>/tmp</code>. This could cause operations issue when a lot of jobs run concurrently. Because of this, you may want to change this by setting <code>java.io.tmp.dir</code> to a different directory.</p>

<h3>Settings To Talk to UNSECURE Hadoop Cluster</h3>

<p>If you are just starting out with Hadoop, chances are you don't have kerberos authentication for your Hadoop. Depending on whether you want to run everything as azkaban user (or whatever user started the azkaban executor server), you can do the following settings:</p>

<ul>
  <li>If you started the executor server with user named azkaban, and you want to run all the jobs as azkaban on Hadoop, just set <code>azkaban.should.proxy=false</code> and <code>obtain.binary.token=false</code></li>
  <li>If you started the executor server with user named azkaban, but you want to run Hadoop jobs as their individual users, you need to set <code>azkaban.should.proxy=true</code> and <code>obtain.binary.token=false</code></li>
</ul>

<h3>Settings To Talk to SECURE Hadoop Cluster</h3>

<p>For secure Hadoop clusters, Azkaban needs its own kerberos keytab to authenticate with KDC. Azkaban job types should acquire necessary Hadoop tokens before user job process starts, and should cancel the tokens after user job finishes. </p>

<p>All job type specific settings should go to their respective plugin conf files. Some of the common settings can go to commonprivate.properties and common.properties.</p>

<p>For instance, Hadoop job types usually require name node tokens and job tracker tokens. These can go to commonpriate.properties.</p>

<h4>Azkaban as proxy user</h4>

<p>The following settings are needed for HadoopSecurityManager to authenticate with KDC:</p>

<pre>
proxy.user=YOUR_AZKABAN_KERBEROS_PRINCIPAL
</pre>

<p>This principal should also be set in core-site.xml in Hadoop conf with corresponding permissions.</p>

<pre>
proxy.keytab.location=KEYTAB_LOCATION
</pre>

<p>One should verify if the proxy user and keytab works with the specified KDC.</p>

<h4>Obtaining tokens for user jobs</h4>

<p>Here are what's common for most Hadoop jobs</p>

<pre>
hadoop.security.manager.class=azkaban.security.HadoopSecurityManager_H_1_0
</pre>

<p>This implementation should work with Hadoop 1.x</p>

<pre>
azkaban.should.proxy=true
obtain.binary.token=true
obtain.namenode.token=true
obtain.jobtracker.token=true
</pre>

<p>Additionally, if your job needs to talk to HCat, for example if you have Hive installed with uses kerbrosed HCat, or your pig job needs to talk to HCat, you will need to set for those Hive job types</p>

<pre>
obtain.hcat.token=true
</pre>

<p>This makes HadoopSecurityManager acquire a HCat token as well.</p>

<h3>Making a New Job Type on Secure Hadoop Cluster</h3>

<p>If you are making a new job type that will talk to Hadoop Cluster, you can use the HadoopSecurityManager to take care of security.</p>

<p>For unsecure Hadoop cluster, there is nothing special that is needed.</p>

<p>For secure Hadoop clusters, there are two ways inlcuded in the hadoopsecuritymanager package:</p>

<ul>
  <li>give the key tab information to user job process. The hadoopsecuritymanager static method takes care of login from that common keytab and proxy to the user. This is convenient for prototyping as there will be a real tgt granted to the user job. The con side is that the user could potentially use the keytab to login and proxy as someone else, which presents a security hole.</li>
  <li>obtain Hadoop tokens prior to user job process start. The job wrapper will pick up these binary tokens inside user job process. The tokens should be explicitly cancelled after user job finishes.</li>
</ul>

<p>By paring properly configured hadoopsecuritymanager with basic job types such as hadoopJava, pig, hive, one can make these job types work with different versions of Hadoop with various security settings.</p>

<p>Included in the azkaban-plugins is the hadoopsecuritymanager for Hadoop-1.x versions. It is not compatible with Hadoop-0.20 and prior versions as Hadoop UGI is not backwards compatible. However, it should not be difficult to implement one that works with them. Going forward, Hadoop UGI is mostly backwards compatible and one only needs to recompile hadoopsecuritymanager package with newer versions of Hadoop.</p>



            <hr>
            <h2 id="hdfs-browser">Azkaban HDFS Browser</h2>

<p>The Azkaban HDFS Browser is a plugin that allows you to view the HDFS FileSystem and decode several file types. It was originally created at LinkedIn to view Avro files, Linkedin's BinaryJson format and text files. As this plugin matures further, we may add decoding of different file types in the future.</p>

<div class="row">
  <div class="col-md-6 col-md-offset-3">
    <a href="./images/hdfsbrowser.png" data-lightbox="hdfs-browser" title="Azkaban HDFS Browser">
      <img class="img-thumbnail img-figure" title="Azkaban HDFS Browser" src="./images/hdfsbrowser.png" ALT="Azkaban HDFS Browser" width="450" />
    </a>
  </div>
</div>

<h3>Setup</h3>

<p>Download the HDFS plugin from <a href="http://azkaban.github.io/downloads.html">the download page</a> and extract it into the web server's plugin's directory. This is often <code>azkaban_web_server_dir/plugins/viewer/</code>.</p>

<h4>Users</h4>

<p>By default, Azkaban HDFS browser does a do-as to impersonate the logged-in user. Often times, data is created and handled by a headless account. To view these files, if user proxy is turned on, then the user can switch to the headless account as long as its validated by the UserManager.</p>

<h4>Settings</h4>

<p>These are properties to configure the HDFS Browser on the AzkabanWebServer. They can be set in <code>azkaban_web_server_dir/plugins/viewer/hdfs/conf/plugin.properties</code>.</p>

<table class="table table-condensed table-striped table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>viewer.name</td>
      <td>The name of this viewer plugin</td>
      <td>HDFS</td>
    </tr>
    <tr>
      <td>viewer.path</td>
      <td>The path to this viewer plugin inside viewer directory.</td>
      <td>hdfs</td>
    </tr>
    <tr>
      <td>viewer.order</td>
      <td>The order of this viewer plugin amongst all viewer plugins.</td>
      <td>1</td>
    </tr>
    <tr>
      <td>viewer.hidden</td>
      <td>Whether this plugin should show up on the web UI.</td>
      <td>false</td>
    </tr>
    <tr>
      <td>viewer.external.classpath</td>
      <td>Extra jars this viewer plugin should load upon init.</td>
      <td>extlib/*</td>
    </tr>
    <tr>
      <td>viewer.servlet.class</td>
      <td>The main servelet class for this viewer plugin. Use <code>azkaban.viewer.hdfs.HdfsBrowserServlet</code> for hdfs browser</td>
      <td></td>
    </tr>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to hadoop clusters. Use <code>azkaban.security.HadoopSecurityManager_H_1_0</code> for hadoop 1.x</td>
      <td></code>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user hadoop accounts on a secure cluster, defaults to false</td>
      <td>false</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and hadoop. Similar to how oozie should be configured, for secure hadoop installations</td>
      <td></td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
      <td></td>
    </tr>
    <tr>
      <td>allow.group.proxy</td>
      <td>Whether to allow users in the same headless user group to view hdfs filesystem as that headless user</td>
      <td>false</td>
    </tr>
  </tbody>
</table>

            <hr>
            <h2 id="jobtype-plugins">Azkaban Jobtype Plugins Configurations</h2>

<p>These are properties to configure the jobtype plugins that are installed with the AzkabanExecutorServer. Note that Azkaban uses the directory structure to infer global settings versus individual jobtype specific settings. Sub-directory names also determine the job type name for running Azkaban instances. </p>

<h3>Introduction</h3>

<p>Jobtype plugins determine how individual jobs are actually run locally or on a remote cluster. It gives great benefits: one can add or change any job type without touching Azkaban core code; one can easily extend Azkaban to run on different hadoop versions or distributions; one can keep old versions around while adding new versions of the same types. However, it is really up to the admin who manages these plugins to make sure they are installed and configured correctly.</p>

<p>Upon AzkabanExecutorServer start up, Azkaban will try to load all the job type plugins it can find. Azkaban will do very simply tests and drop the bad ones. One should always try to run some test jobs to make sure the job types really work as expected.</p>

<h3>Global Properties</h3>

<p>One can pass global settings to all job types, including cluster dependent settings that will be used by all job types. These settings can also be specified in each job type's own settings as well.</p>

<h3>Private settings</h3>

<p>One can pass global settings that are needed by job types but should not be accessible by user code in <code>commonprivate.properties</code>. For example, the following settings are often needed for a hadoop cluster:</p>

<table class="table table-bordered table-striped table-condensed">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The hadoopsecuritymanager that handles talking to a hadoop cluseter. Use <code>azkaban.security.HadoopSecurityManager_H_1_0</code> for 1.x versions</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user hadoop accounts, or run as the Azkaban user itself, defaults to <code>true</code></td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and hadoop. Similar to how oozie should be configured, for secure hadoop installations</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
    </tr>
    <tr>
      <td>jobtype.global.classpath</td>
      <td>The jars or xml resources every job type should have on their classpath. (e.g. <code>${hadoop.home}/hadoop-core-1.0.4.jar,${hadoop.home}/conf</code>)</td>
    </tr>
    <tr>
      <td>jobtype.global.jvm.args</td>
      <td>The jvm args that every job type should have to jvm.</td>
    </tr>
    <tr>
      <td>hadoop.home</td>
      <td>The <code>$HADOOP_HOME</code> setting.</td>
    </tr>
  </tbody>
</table>

<h3>Public settings</h3>

<p>One can pass global settings that are needed by job types and can be visible by user code, in <code>common.properties</code>. For example, <code>hadoop.home</code> should normally be passed along to user programs.</p>

<h3>Settings for individual job types</h3>

<p>
  In most cases, there is no extra settings needed for job types to work, other than variables like <code>hadoop.home</code>, <code>pig.home</code>, <code>hive.home</code>, etc. However, it is also where most of the customizations come from. For example, one can configure a two Java job types with the same jar resources but with different hadoop configurations, thereby submitting pig jobs to different clusters. One can also configure pig job with pre-registered jars and namespace imports for specific organizations. Also to be noted: in the list of common job type plugins, we have included different pig versions. The admin needs to make a soft link to one of them, such as
</p>

<pre>
$ ln -s pig-0.10.1 pig
</pre>

<p>so that the users can use a default "pig" type.</p>


          </div>

          <div class="docs-section">
            <div class="page-header">
  <h1 id="job-types">Jobtypes</h1>
</div>

<p>Azkaban job type plugin design provides great flexibility for developers to create any type of job executors which can work with essentially all types of systems -- all managed and triggered by the core Azkaban work flow management.</p>

<p>Here we provide a common set of plugins that should be useful to most hadoop related use cases, as well as sample job packages. Most of these job types are being used in LinkedIn's production clusters, only with different configurations. We also give a simple guide how one can create new job types, either from scratch or by extending the old ones.</p>

            <hr>
            <h2 id="command-type">Command Job Type (built-in)</h2>

<p>The command job type is one of the basic built-in types. It runs multiple UNIX commands using java processbuilder.  Upon execution, Azkaban spawns off a process to run the command.</p>

<h3>How To Use</h3>

<p>One can run one or multiple commands within one command job. Here is what is needed:</p>

<table class="table table-bordered table-striped table-condensed">
  <thead>
    <tr>
      <th>Type</th>
      <th>Command</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>command</td>
      <td>The full command to run</td>
    </tr>
  </tbody>
</table>

<p>For multiple commands, do it like <code>command.1, command.2</code>, etc.</p>

<div class="bs-callout bs-callout-info">
  <h4>Sample Job Package</h4>

  <p>Here is a sample job package, just to show how it works:</p>
  <a class="btn btn-primary" href="https://s3.amazonaws.com/azkaban2/azkaban2/samplejobs/command.zip" target="_blank"><span class="glyphicon glyphicon-download-alt"></span> Download command.zip</a> (Uploaded May 13, 2013)
</div>

            <hr>
            <h2 id="hadoopshell-type">HadoopShell Job Type</h2>

<p>In large part, this is the same <code>Command</code> type. The difference is its ability to talk to a Hadoop cluster securely, via Hadoop tokens.</p>

<p>The HadoopShell job type is one of the basic built-in types. It runs multiple UNIX commands using java processbuilder.  Upon execution, Azkaban spawns off a process to run the command.</p>

<h3>How To Use</h3>

<p>The <code>HadoopShell</code> job type talks to a secure cluster via Hadoop tokens. The admin should specify <code>obtain.binary.token=true</code> if the Hadoop cluster security is turned on. Before executing a job, Azkaban will obtain name node token and job tracker tokens for this job. These tokens will be written to a token file, to be picked up by user job process during its execution. After the job finishes, Azkaban takes care of canceling these tokens from name node and job tracker. </p>

<p>Since Azkaban only obtains the tokens at the beginning of the job run, and does not requesting new tokens or renew old tokens during the execution, it is important that the job does not run longer than configured token life.</p>

<p>One can run one or multiple commands within one command job. Here is what is needed:</p>

<table class="table table-bordered table-striped table-condensed">
  <thead>
    <tr>
      <th>Type</th>
      <th>Command</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>command</td>
      <td>The full command to run</td>
    </tr>
  </tbody>
</table>

<p>For multiple commands, do it like <code>command.1, command.2</code>, etc.</p>

<p>Here are some common configurations that make a <code>hadoopShell</code> job for a user:</p>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>hadoopShell</code></td>
    </tr>
    <tr>
      <td>dependencies</td>
      <td>The other jobs in the flow this job is dependent upon.</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The Hadoop user this job should run under.</td>
    </tr>
    <tr>
      <td>hadoop-inject.FOO</td>
      <td>FOO is automatically added to the Configuration of any Hadoop job launched.</td>
    </tr>
  </tbody>
</table>

<p>Here are what's needed and normally configured by the admin:</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to Hadoop clusters.</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user Hadoop accounts.</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and Hadoop, for secure clusters.</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
    </tr>
    <tr>
      <td>obtain.binary.token</td>
      <td>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
    </tr>
  </tbody>
</table>

            <hr>
            <h2 id="java-type">Java Job Type</h2>

<p>The <code>java</code> job type was widely used in the original Azkaban as a built-in type. It is no longer a built-in type in Azkaban2. The <code>javaprocess</code> is still built-in in Azkaban2. The main difference between <code>java</code> and <code>javaprocess</code> job types are: </p>

<ol>
  <li><code>javaprocess</code> runs user program that has a "main" method, <code>java</code> runs Azkaban provided main method which invokes user program "run" method.</li>
  <li>Azkaban can do the setup, such as getting Kerberos ticket or requesting Hadoop tokens in the provided main in <code>java</code> type, whereas in <code>javaprocess</code> user is responsible for everything.</li>
</ol>

<p>As a result, most users use <code>java</code> type for running anything that talks to Hadoop clusters. That usage should be replaced by <code>hadoopJava</code> type now, which is secure. But we still keep <code>java</code> type in the plugins for backwards compatibility.</p>

<h3>How to Use</h3>

<p>Azkaban spawns a local process for the java job type that runs user programs. It is different from the "javaprocess" job type in that Azkaban already provides a <code>main</code> method, called <code>JavaJobRunnerMain</code>. Inside <code>JavaJobRunnerMain</code>, it looks for the <code>run</code> method which can be specified by <code>method.run</code> (default is <code>run</code>). User can also specify a <code>cancel</code> method in the case the user wants to gracefully terminate the job in the middle of the run.</p>

<p>For the most part, using <code>java</code> type should be no different from <code>hadoopJava</code>. </p>

<div class="bs-callout bs-callout-info">
  <h4>Sample Job</h4>
  <p>Please refer to the <a href="#hadoopjava-type"><code>hadoopJava</code> type</a>.</p>
</div>

            <hr>
            <h2 id="hadoopjava-type">hadoopJava Type</h2>

<p>In large part, this is the same <code>java</code> type. The difference is its ability to talk to a Hadoop cluster securely, via Hadoop tokens. Most Hadoop job types can be created by running a hadoopJava job, such as Pig, Hive, etc.</p>

<h3>How To Use</h3>

<p>The <code>hadoopJava</code> type runs user java program after all. Upon execution, it tries to construct an object that has the constructor signature of <code>constructor(String, Props)</code> and runs its <code>run</code> method. If user wants to cancel the job, it tries the user defined <code>cancel</code> method before doing a hard kill on that process.</p>

<p>The <code>hadoopJava</code> job type talks to a secure cluster via Hadoop tokens. The admin should specify <code>obtain.binary.token=true</code> if the Hadoop cluster security is turned on. Before executing a job, Azkaban will obtain name node token and job tracker tokens for this job. These tokens will be written to a token file, to be picked up by user job process during its execution. After the job finishes, Azkaban takes care of canceling these tokens from name node and job tracker. </p>

<p>Since Azkaban only obtains the tokens at the beginning of the job run, and does not requesting new tokens or renew old tokens during the execution, it is important that the job does not run longer than configured token life.</p>

<p>If there are multiple job submissions inside the user program, the user should also take care not to have a single MR step cancel the tokens upon completion, thereby failing all other MR steps when they try to authenticate with Hadoop services.</p>

<p>In many cases, it is also necessary to add the following code to make sure user program picks up the Hadoop tokens in "conf" or "jobconf" like the following:</p>

<pre class="code">
// Suppose this is how one gets the conf
Configuration conf = new Configuration();

if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
    conf.set("mapreduce.job.credentials.binary", System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
}
</pre>

<p>Here are some common configurations that make a <code>hadoopJava</code> job for a user:</p>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>hadoopJava</code></td>
    </tr>
    <tr>
      <td>job.class</td>
      <td>The fully qualified name of the user job class.</td>
    </tr>
    <tr>
      <td>classpath</td>
      <td>The resources that should be on the execution classpath, accessible to the local filesystem.</td>
    </tr>
    <tr>
      <td>main.args</td>
      <td>Main arguments passed to user program.</td>
    </tr>
    <tr>
      <td>dependencies</td>
      <td>The other jobs in the flow this job is dependent upon.</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The Hadoop user this job should run under.</td>
    </tr>
    <tr>
      <td>method.run</td>
      <td>The run method, defaults to <em>run()</em></td>
    </tr>
    <tr>
      <td>method.cancel</td>
      <td>The cancel method, defaults to <em>cancel()</em></td>
    </tr>
    <tr>
      <td>getJobGeneratedProperties</td>
      <td>The method user should implement if the output properties should be picked up and passed to the next job.</td>
    </tr>
    <tr>
      <td>jvm.args</td>
      <td>The <code>-D</code> for the new jvm process</td>
    </tr>
    <tr>
      <td>hadoop-inject.FOO</td>
      <td>FOO is automatically added to the Configuration of any Hadoop job launched.</td>
    </tr>
  </tbody>
</table>

<p>Here are what's needed and normally configured by the admin:</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to Hadoop clusters.</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user Hadoop accounts.</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and Hadoop, for secure clusters.</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
    </tr>
    <tr>
      <td>hadoop.home</td>
      <td>The Hadoop home where the jars and conf resources are installed.</td>
    </tr>
    <tr>
      <td>jobtype.classpath</td>
      <td>The items that every such job should have on its classpath.</td>
    </tr>
    <tr>
      <td>jobtype.class</td>
      <td>Should be set to <code>azkaban.jobtype.HadoopJavaJob</code></td>
    </tr>
    <tr>
      <td>obtain.binary.token</td>
      <td>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
    </tr>
  </tbody>
</table>

<p>Since Azkaban job types are named by their directory names, the admin should also make those naming public and consistent.</p>

<div class="bs-callout bs-callout-info">
  <h4>Sample Job Package</h4>
  <p>Here is a sample job package that does a word count. It relies on a Pig job to first upload the text file onto HDFS. One can also manually upload a file and run the word count program alone.The source code is in <code>azkaban-plugins/plugins/jobtype/src/azkaban/jobtype/examples/java/WordCount.java</code></p>
  <a class="btn btn-primary" href="https://s3.amazonaws.com/azkaban2/azkaban2/samplejobs/java-wc.zip" target="_blank"><span class="glyphicon glyphicon-download-alt"></span> Download java-wc.zip</a> (Uploaded May 13, 2013)
</div>

            <hr>
            <h2 id="pig-type">Pig Type</h2>

<p>Pig type is for running Pig jobs. In the <code>azkaban-plugins</code> repo, we have included Pig types from pig-0.9.2 to pig-0.11.0. It is up to the admin to alias one of them as the <code>pig</code> type for Azkaban users.</p>

<p>Pig type is built on using hadoop tokens to talk to secure Hadoop clusters. Therefore, individual Azkaban Pig jobs are restricted to run within the token's lifetime, which is set by Hadoop admins. It is also important that individual MR step inside a single Pig script doesn't cancel the tokens upon its completion. Otherwise, all following steps will fail on authentication with job tracker or name node.</p>

<p>Vanilla Pig types don't provide all udf jars. It is often up to the admin who sets up Azkaban to provide a pre-configured Pig job type with company specific udfs registered and name space imported, so that the users don't need to provide all the jars and do the configurations in their specific Pig job conf files.</p>

<h3>How to Use</h3>

<p>The Pig job runs user Pig scripts. It is important to remember, however, that running any Pig script might require a number of dependency libraries that need to be placed on local Azkaban job classpath, or be registered with Pig and carried remotely, or both. By using classpath settings, as well as <code>pig.additional.jars</code> and <code>udf.import.list</code>, the admin can create a Pig job type that has very different default behavior than the most basic "pig" type. Pig jobs talk to a secure cluster via hadoop tokens. The admin should specify <code>obtain.binary.token=true</code> if the hadoop cluster security is turned on. Before executing a job, Azkaban will obtain name node and job tracker tokens for this job. These tokens will be written to a token file, which will be picked up by user job process during its execution. For Hadoop 1 (<code>HadoopSecurityManager_H_1_0</code>), after the job finishes, Azkaban takes care of canceling these tokens from name node and job tracker. In Hadoop 2 (<code>HadoopSecurityManager_H_2_0</code>), due to issues with tokens being canceled prematurely, Azkaban does not cancel the tokens.</p>

<p>Since Azkaban only obtains the tokens at the beginning of the job run, and does not request new tokens or renew old tokens during the execution, it is important that the job does not run longer than configured token life. It is also important that individual MR step inside a single Pig script doesn't cancel the tokens upon its completion. Otherwise, all following steps will fail on authentication with hadoop services. In Hadoop 2, you may need to set <code>-Dmapreduce.job.complete.cancel.delegation.tokens=false</code> to prevent tokens from being canceled prematurely.</p>

<p>Here are the common configurations that make a Pig job for a <em>user</em>:</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>pig</code></td>
    </tr>
    <tr>
      <td>pig.script</td>
      <td>The Pig script location. e.g. <code>src/wordcountpig.pig</code></td>
    </tr>
    <tr>
      <td>classpath</td>
      <td>The resources that should be on the execution classpath, accessible to the local filesystem.</td>
    </tr>
    <tr>
      <td>dependencies</td>
      <td>The other jobs in the flow this job is dependent upon.</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The hadoop user this job should run under.</td>
    </tr>
    <tr>
      <td>pig.home</td>
      <td>The Pig installation directory. Can be used to override the default set by Azkaban.</td>
    </tr>
    <tr>
      <td>param.SOME_PARAM</td>
      <td>Equivalent to Pig's <code>-param</code></td>
    </tr>
    <tr>
      <td>use.user.pig.jar</td>
      <td>If true, will use the user-provided Pig jar to launch the job. If false, the Pig jar provided by Azkaban will be used. Defaults to false.</td>
    </tr>
    <tr>
      <td>hadoop-inject.FOO</td>
      <td>FOO is automatically added to the Configuration of any Hadoop job launched.</td>
    </tr>
  </tbody>
</table>

<p>Here are what's needed and normally configured by the admin:</p>

<table class="table table-condensed table-bordered table-striped">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to hadoop clusters.</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user hadoop accounts.</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and hadoop, for secure clusters.</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
    </tr>
    <tr>
      <td>hadoop.home</td>
      <td>The hadoop home where the jars and conf resources are installed.</td>
    </tr>
    <tr>
      <td>jobtype.classpath</td>
      <td>The items that every such job should have on its classpath.</td>
    </tr>
    <tr>
      <td>jobtype.class</td>
      <td>Should be set to <code>azkaban.jobtype.HadoopJavaJob</code></td>
    </tr>
    <tr>
      <td>obtain.binary.token</td>
      <td>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
    </tr>
  </tbody>
</table>

<p>Dumping MapReduce Counters: this is useful in the case where a Pig script uses UDFs, which may add a few custom MapReduce counters</p>

<table class="table table-condensed table-bordered table-striped">
  <thead>
    <tr>
      <th>Parameter</th>
      <th> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>pig.dump.hadoopCounter</td>
      <td>Setting the value of this parameter to true will trigger the dumping of MapReduce counters for each of the generated MapReduce job generated by the Pig script.</td>
    </tr>
    </tbody>
</table>

<p>Since Pig jobs are essentially Java programs, the configurations for Java jobs could also be set.</p>

<p>Since Azkaban job types are named by their directory names, the admin should also make those naming public and consistent. For example, while there are multiple versions of Pig job types, the admin can link one of them as <code>pig</code> for default Pig type. Experimental Pig versions can be tested in parallel with a different name and can be promoted to default Pig type if it is proven stable. In LinkedIn, we also provide Pig job types that have a number of useful udf libraries, including datafu and LinkedIn specific ones, pre-registered and imported, so that users in most cases will only need Pig scripts in their Azkaban job packages.</p>

<div class="bs-callout bs-callout-info">
  <h4>Sample Job Package</h4>
  <p>Here is a sample job package that does word count. It assumes you have hadoop installed and gets some dependency jars from <code>$HADOOP_HOME</code>:</p>
  <a class="btn btn-primary" href="https://s3.amazonaws.com/azkaban2/azkaban2/samplejobs/pig-wc.zip" target="_blank"><span class="glyphicon glyphicon-download-alt"></span> Download pig-wc.zip</a> (Uploaded May 13, 2013)
</div>

            <hr>
            <h2 id="hive-type">Hive Type</h2>

<p>The <code>hive</code> type is for running Hive jobs. In the <a href="https://github.com/azkaban/azkaban-plugins" target="blank">azkaban-plugins</a> repo, we have included hive type based on hive-0.8.1. It should work for higher version Hive versions as well. It is up to the admin to alias one of them as the <code>hive</code> type for Azkaban users.</p>

<p>The <code>hive</code> type is built using Hadoop tokens to talk to secure Hadoop clusters. Therefore, individual Azkaban Hive jobs are restricted to run within the token's lifetime, which is set by Hadoop admin. It is also important that individual MR step inside a single Pig script doesn't cancel the tokens upon its completion. Otherwise, all following steps will fail on authentication with the JobTracker or NameNode.</p>

<h3>How to Use</h3>

<p>The Hive job runs user Hive queries. The Hive job type talks to a secure cluster via Hadoop tokens. The admin should specify <code>obtain.binary.token=true</code> if the Hadoop cluster security is turned on. Before executing a job, Azkaban will obtain NameNode and JobTracker tokens for this job. These tokens will be written to a token file, which will be picked up by user job process during its execution. After the job finishes, Azkaban takes care of canceling these tokens from NameNode and JobTracker.</p>

<p>Since Azkaban only obtains the tokens at the beginning of the job run, and does not request new tokens or renew old tokens during the execution, it is important that the job does not run longer than configured token life. It is also important that individual MR step inside a single Pig script doesn't cancel the tokens upon its completion. Otherwise, all following steps will fail on authentication with Hadoop services.</p>

<p>Here are the common configurations that make a <code>hive</code> job for single line Hive query:</p>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th class="parameter">Parameter</th>
      <th class="description"> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>hive</code></td>
    </tr>
    <tr>
      <td>azk.hive.action</td>
      <td>use <code>execute.query</code></td>
    </tr>
    <tr>
      <td>hive.query</td>
      <td>Used for single line hive query.</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The hadoop user this job should run under.</td>
    </tr>
  </tbody>
</table>

<p>Specify these for a multi-line Hive query:</p>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th class="parameter">Parameter</th>
      <th class="description">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>hive</code></td>
    </tr>
    <tr>
      <td>azk.hive.action</td>
      <td>use <code>execute.query</code></td>
    </tr>
    <tr>
      <td>hive.query.01</td>
      <td>fill in the individual hive queries, starting from 01</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The Hadoop user this job should run under.</td>
    </tr>
  </tbody>
</table>

<p>Specify these for query from a file:</p>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>hive</code></td>
    </tr>
    <tr>
      <td>azk.hive.action</td>
      <td>use <code>execute.query</code></td>
    </tr>
    <tr>
      <td>hive.query.file</td>
      <td>location of the query file</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The Hadoop user this job should run under.</td>
    </tr>
  </tbody>
</table>

<p>Here are what's needed and normally configured by the admin. The following properties go into private.properties:</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to hadoop clusters.</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user hadoop accounts.</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and hadoop, for secure clusters.</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
    </tr>
    <tr>
      <td>hadoop.home</td>
      <td>The hadoop home where the jars and conf resources are installed.</td>
    </tr>
    <tr>
      <td>jobtype.classpath</td>
      <td>The items that every such job should have on its classpath.</td>
    </tr>
    <tr>
      <td>jobtype.class</td>
      <td>Should be set to <code>azkaban.jobtype.HadoopJavaJob</code></td>
    </tr>
    <tr>
      <td>obtain.binary.token</td>
      <td>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
    </tr>
    <tr>
      <td>hive.aux.jars.path</td>
      <td>Where to find auxiliary library jars</td>
    </tr>
    <tr>
      <td>env.HADOOP_HOME</td>
      <td><code>$HADOOP_HOME</code></td>
    </tr>
    <tr>
      <td>env.HIVE_HOME</td>
      <td><code>$HIVE_HOME</code></td>
    </tr>
    <tr>
      <td>env.HIVE_AUX_JARS_PATH</td>
      <td><code>${hive.aux.jars.path}</code></td>
    </tr>
    <tr>
      <td>hive.home</td>
      <td><code>$HIVE_HOME</code></td>
    </tr>
    <tr>
      <td>hive.classpath.items</td>
      <td>Those that needs to be on hive classpath, include the conf directory</td>
    </tr>
  </tbody>
</table>

<p>These go into plugin.properties</p>

<table class="table table-bordered table-striped table-condensed">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style='text-align: left;'>job.class</td>
      <td style='text-align: left;'><code>azkaban.jobtype.hiveutils.azkaban.HiveViaAzkaban</code></td>
    </tr>
    <tr>
      <td style='text-align: left;'>hive.aux.jars.path</td>
      <td style='text-align: left;'>Where to find auxiliary library jars</td>
    </tr>
    <tr>
      <td style='text-align: left;'>env.HIVE_HOME</td>
      <td style='text-align: left;'><code>$HIVE_HOME</code></td>
    </tr>
    <tr>
      <td style='text-align: left;'>env.HIVE_AUX_JARS_PATH</td>
      <td style='text-align: left;'><code>${hive.aux.jars.path}</code></td>
    </tr>
    <tr>
      <td style='text-align: left;'>hive.home</td>
      <td style='text-align: left;'><code>$HIVE_HOME</code></td>
    </tr>
    <tr>
      <td style='text-align: left;'>hive.jvm.args</td>
      <td style='text-align: left;'><code>-Dhive.querylog.location=.</code> <code>-Dhive.exec.scratchdir=YOUR_HIVE_SCRATCH_DIR</code> <code>-Dhive.aux.jars.path=${hive.aux.jars.path}</code></td>
    </tr>
  </tbody>
</table>

<p>Since hive jobs are essentially java programs, the configurations for Java jobs could also be set.</p>

<div class="bs-callout bs-callout-info">
  <h4>Sample Job Package</h4>
  <p>Here is a sample job package. It assumes you have hadoop installed and gets some dependency jars from <code>$HADOOP_HOME</code>. It also assumes you have Hive installed and configured correctly, including setting up a MySQL instance for Hive Metastore.</p>
  <a class="btn btn-primary" href="https://s3.amazonaws.com/azkaban2/azkaban2/samplejobs/hive.zip"><span class="glyphicon glyphicon-download-alt"></span>Download hive.zip</a> (Uploaded May 13, 2013)</p>
</div>

            <hr>
            <h2 id="new-hive-type">New Hive Jobtype</h2>

<p>We've added a new Hive jobtype whose jobtype class is <code>azkaban.jobtype.HadoopHiveJob</code>. The configurations have changed from the old Hive jobtype.</p>

<p>Here are the configurations that a user can set:</p>

<table class="table table-striped table-condensed table-bordered">
  <thead>
    <tr>
      <th class="parameter">Parameter</th>
      <th class="description"> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>hive</code></td>
    </tr>
    <tr>
      <td>hive.script</td>
      <td>The relative path of your Hive script inside your Azkaban zip</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The hadoop user this job should run under.</td>
    </tr>
    <tr>
      <td>hiveconf.FOO</td>
      <td>FOO is automatically added as a hiveconf variable. You can reference it in your script using ${hiveconf:FOO}. These variables also get added to the configuration of any launched Hadoop jobs.</td>
    </tr>
    <tr>
      <td>hivevar.FOO</td>
      <td>FOO is automatically added as a hivevar variable. You can reference it in your script using ${hivevar:FOO}. These variables are NOT added to the configuration of launched Hadoop jobs.</td>
    </tr>
    <tr>
      <td>hadoop-inject.FOO</td>
      <td>FOO is automatically added to the Configuration of any Hadoop job launched.</td>
    </tr>
  </tbody>
</table>

<p>Here are what's needed and normally configured by the admin. The following properties go into private.properties (or into ../commonprivate.properties):</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to hadoop clusters.</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user hadoop accounts.</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and hadoop, for secure clusters.</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
    </tr>
    <tr>
      <td>hadoop.home</td>
      <td>The hadoop home where the jars and conf resources are installed.</td>
    </tr>
    <tr>
      <td>jobtype.classpath</td>
      <td>The items that every such job should have on its classpath.</td>
    </tr>
    <tr>
      <td>jobtype.class</td>
      <td>Should be set to <code>azkaban.jobtype.HadoopHiveJob</code></td>
    </tr>
    <tr>
      <td>obtain.binary.token</td>
      <td>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
    </tr>
    <tr>
      <td>obtain.hcat.token</td>
      <td>Whether Azkaban should request HCatalog/Hive Metastore tokens. If true, the HadoopSecurityManager will acquire an HCatalog token.</td>
    </tr>
    <tr>
      <td>hive.aux.jars.path</td>
      <td>Where to find auxiliary library jars</td>
    </tr>
    <tr>
      <td>hive.home</td>
      <td><code>$HIVE_HOME</code></td>
    </tr>
  </tbody>
</table>

<p>These go into plugin.properties (or into ../common.properties):</p>

<table class="table table-bordered table-striped table-condensed">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style='text-align: left;'>hive.aux.jars.path</td>
      <td style='text-align: left;'>Where to find auxiliary library jars</td>
    </tr>
    <tr>
      <td style='text-align: left;'>hive.home</td>
      <td style='text-align: left;'><code>$HIVE_HOME</code></td>
    </tr>
    <tr>
      <td style='text-align: left;'>jobtype.jvm.args</td>
      <td style='text-align: left;'><code>-Dhive.querylog.location=.</code> <code>-Dhive.exec.scratchdir=YOUR_HIVE_SCRATCH_DIR</code> <code>-Dhive.aux.jars.path=${hive.aux.jars.path}</code></td>
    </tr>
  </tbody>
</table>

<p>Since hive jobs are essentially java programs, the configurations for Java jobs can also be set.</p>
            <hr>
            <h2 id="common-configurations">Common Configurations</h2>

<p>This section lists out the configurations that are common to all job types</p>

<h3>other_namenodes</h3>
<p>This job property is useful for jobs that need to read data from or write data to more than one Hadoop NameNode. By default Azkaban requests a HDFS_DELEGATION_TOKEN on behalf of the job for the cluster that Azkaban is configured to run on.  When this property is present, Azkaban will try request a HDFS_DELEGATION_TOKEN for each of the specified HDFS NameNodes.</p>
<p>The value of this propety is in the form of comma separated list of NameNode URLs.</p>
<p>For example: <b>other_namenodes=webhdfs://host1:50070,hdfs://host2:9000</b></p>

<h3>HTTP Job Callback</h3>

<p>The purpose of this feature to allow Azkaban to notify external systems via an HTTP upon the completion of a job. The new properties are in the following format:</p>

<ul>
<li><b>job.notification.&lt;status&gt;.&lt;sequence number&gt;.url</b></li>
<li><b>job.notification.&lt;status&gt;.&lt;sequence number&gt;.method</b></li>
<li><b>job.notification.&lt;status&gt;.&lt;sequence number&gt;.body</b></li>
<li><b>job.notification.&lt;status&gt;.&lt;sequence number&gt;.headers</b></li>
</ul>

<h4>Supported values for <b>status</b></h4>
<ul>
  <li><b>started</b>: when a job is started</li>
  <li><b>success</b>: when a job is completed successfully</li>
  <li><b>failure</b>: when a job failed</li>
  <li><b>completed</b>: when a job is either successfully completed or failed</li>
</ul>

<h4>Number of callback URLs</h4>
<p>The maximum # of callback URLs per job is 3. So the &lt;sequence number&gt; can go up from 1 to 3.
   If a gap is detected, only the ones before the gap is used.</p>

<h4>HTTP Method</h4>
<p>The supported method are <b>GET</b> and <b>POST</b>.  The default method is <b>GET</b></p>

<h4>Headers</h4>
<p>Each job callback URL can optional specify headers in the following format</p>
<b>job.notification.&lt;status&gt;.&lt;sequence number&gt;.headers</b>=&lt;name&gt;:&lt;value&gt;\r\n&lt;name&gt;:&lt;value&gt;
<p>The delimiter for each header is '\r\n' and delimiter between header name and value is ':'</p>
<p>The headers are applicable for both GET and POST job callback URLs.</p>

<h4>Job Context Information</h4>

<p>It is often desirable to include some dynamic context information about the job in the URL or POST request body, such as status, job name, flow name, execution id and project name.
If the URL or POST request body contains any of the following tokens, they will be replaced with the actual values by Azkabn before making the HTTP callback is made. The value of each token will be HTTP encoded.</p>
<ul>
<li><b>?{server}</b> - Azkaban host name and port
<li><b>?{project}</b>
<li><b>?{flow}</b>
<li><b>?{executionId}</b>
<li><b>?{job}</b>
<li><b>?{status}</b> - possible values are started, failed, succeeded
</ul>

The value of these tokens will be HTTP encoded if they are on the URL, but will not be encoded when they are in the HTTP body.

<h4>Examples</h4>
<p>GET HTTP Method<p>
<ul>
  <li>job.notification.started.1.url=http://abc.com/api/v2/message?text=wow!!&job=?{job}&status=?{status}
  <li>job.notification.completed.1.url=http://abc.com/api/v2/message?text=wow!!&job=?{job}&status=?{status}
  <li>job.notification.completed.2.url=http://abc.com/api/v2/message?text=yeah!!
</ul>

<p>POST HTTP Method</p>
<ul>
  <li>job.notification.started.1.url=http://abc.com/api/v1/resource
  <li>job.notification.started.1.method=POST
  <li>job.notification.started.1.body={"type":"workflow", "source":"Azkaban", "content":"{server}:?{project}:?{flow}:?{executionId}:?{job}:?{status}"}
    <li>job.notification.started.1.headers=Content-type:application/json
</ul>


            <hr>
            <h2 id="voldemortbuildandpush-type">VoldemortBuildandPush Type</h2>

<p>Pushing data from hadoop to voldemort store used to be entirely in java. This created lots of problems, mostly due to users having to keep track of jars and dependencies and keep them up-to-date. We created the <code>VoldemortBuildandPush</code> job type to address this problem. Jars and dependencies are now managed by admins; absolutely no jars or java code are required from users.</p>

<h3>How to Use</h3>

<p>This is essentially a hadoopJava job, with all jars controlled by the admins. User only need to provide a .job file for the job and specify all the parameters. The following needs to be specified:</p>

<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>type</td>
      <td>The type name as set by the admin, e.g. <code>VoldemortBuildandPush</code></td>
    </tr>
    <tr>
      <td>push.store.name</td>
      <td>The voldemort push store name</td>
    </tr>
    <tr>
      <td>push.store.owners</td>
      <td>The push store owners</td>
    </tr>
    <tr>
      <td>push.store.description</td>
      <td>Push store description</td>
    </tr>
    <tr>
      <td>build.input.path</td>
      <td>Build input path on hdfs</td>
    </tr>
    <tr>
      <td>build.output.dir</td>
      <td>Build output path on hdfs</td>
    </tr>
    <tr>
      <td>build.replication.factor</td>
      <td>replication factor number</td>
    </tr>
    <tr>
      <td>user.to.proxy</td>
      <td>The hadoop user this job should run under.</td>
    </tr>
    <tr>
      <td>build.type.avro</td>
      <td>if build and push avro data, true, otherwise, false</td>
    </tr>
    <tr>
      <td>avro.key.field</td>
      <td>if using Avro data, key field</td>
    </tr>
    <tr>
      <td>avro.value.field</td>
      <td>if using Avro data, value field</td>
    </tr>
  </tbody>
</table>

<p>Here are what's needed and normally configured by the admn (always put common properties in <code>commonprivate.properties</code> and <code>common.properties</code> for all job types).</p>

<p>These go into <code>private.properties</code>:</p>

<table class="table table-condensed table-bordered table-striped">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'> Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>hadoop.security.manager.class</td>
      <td>The class that handles talking to hadoop clusters.</td>
    </tr>
    <tr>
      <td>azkaban.should.proxy</td>
      <td>Whether Azkaban should proxy as individual user hadoop accounts.</td>
    </tr>
    <tr>
      <td>proxy.user</td>
      <td>The Azkaban user configured with kerberos and hadoop, for secure clusters.</td>
    </tr>
    <tr>
      <td>proxy.keytab.location</td>
      <td>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified <code>proxy.user</code></td>
    </tr>
    <tr>
      <td>hadoop.home</td>
      <td>The hadoop home where the jars and conf resources are installed.</td>
    </tr>
    <tr>
      <td>jobtype.classpath</td>
      <td>The items that every such job should have on its classpath.</td>
    </tr>
    <tr>
      <td>jobtype.class</td>
      <td>Should be set to <code>azkaban.jobtype.HadoopJavaJob</code></td>
    </tr>
    <tr>
      <td>obtain.binary.token</td>
      <td>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
    </tr>
    <tr>
      <td>azkaban.no.user.classpath</td>
      <td>Set to true such that Azkaban doesn't pick up user supplied jars.</td>
    </tr>
  </tbody>
</table>

<p>These go into <code>plugin.properties</code>:</p>

<table class="table table-condensed table-bordered table-striped">
  <thead>
    <tr>
      <th class='parameter'>Parameter</th>
      <th class='description'>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>job.class</td>
      <td><code>voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob</code></td>
    </tr>
    <tr>
      <td>voldemort.fetcher.protocol</td>
      <td><code>webhdfs</code></td>
    </tr>
    <tr>
      <td>hdfs.default.classpath.dir</td>
      <td>HDFS location for distributed cache</td>
    </tr>
    <tr>
      <td>hdfs.default.classpath.dir.enable</td>
      <td>set to true if using distributed cache to ship dependency jars</td>
    </tr>
  </tbody>
</table>

<div class="bs-callout bs-callout-info">
  <h4>For more information</h4>
  <p>Please refer to <a href="http://project-voldemort.com/voldemort" target="_blank">Voldemort project site</a> for more info.</p>
</div>

            <hr>
            <h2 id="create-job-types">Create Your Own Jobtypes</h2>

<p>With plugin design of Azkaban job types, it is possible to extend Azkaban for various system environments. You should be able to execute any job under the same Azkaban work flow management and scheduling.</p>

<p>Creating new job types is often times very easy. Here are several ways one can do it:</p>

<h3>New Types with only Configuration Changes</h3>

<p>One doesn't always need to write java code to create job types for end users. Often times, configuration changes of existing job types would create significantly different behavior to the end users. For example, in LinkedIn, apart from the <em>pig</em> types, we also have <em>pigLi</em> types that come with all the useful library jars pre-registered and imported. This way, normal users only need to provide their pig scripts, and the their own udf jars to Azkaban. The pig job should run as if it is run on the gateway machine from pig grunt. In comparison, if users are required to use the basic <em>pig</em> job types, they will need to package all the necessary jars in the Azkaban job package, and do all the register and import by themselves, which often poses some learning curve for new pig/Azkaban users.</p>

<p>The same practice applies to most other job types. Admins should create or tailor job types to their specific company needs or clusters. </p>

<h3>New Types Using Existing Job Types</h3>

<p>If one needs to create a different job type, a good starting point is to see if this can be done by using an existing job type. In hadoop land, this most often means the hadoopJava type. Essentially all hadoop jobs, from the most basic mapreduce job, to pig, hive, crunch, etc, are java programs that submit jobs to hadoop clusters. It is usually straight forward to create a job type that takes user input and runs a hadoopJava job.</p>

<p>For example, one can take a look at the VoldemortBuildandPush job type. It will take in user input such as which cluster to push to, voldemort store name, etc, and runs hadoopJava job that does the work. For end users though, this is a VoldemortBuildandPush job type with which they only need to fill out the <code>.job</code> file to push data from hadoop to voldemort stores.</p>

<p>The same applies to the hive type.</p>

<h3>New Types by Extending Existing Ones</h3>

<p>For the most flexibility, one can always build new types by extending the existing ones. Azkaban uses reflection to load job types that implements the <code>job</code> interface, and tries to construct a sample object upon loading for basic testing. When executing a real job, Azkaban calls the <code>run</code> method to run the job, and <code>cancel</code> method to cancel it. </p>

<p>For new hadoop job types, it is important to use the correct <code>hadoopsecuritymanager</code> class, which is also included in <code>azkaban-plugins</code> repo. This class handles talking to the hadoop cluster, and if needed, requests tokens for job execution or for name node communication.</p>

<p>For better security, tokens should be requested in Azkaban main process and be written to a file. Before executing user code, the job type should implement a wrapper that picks up the token file, set it in the <code>Configuration</code> or <code>JobConf</code> object. Please refer to <code>HadoopJavaJob</code> and <code>HadoopPigJob</code> to see example usage.</p>

            <hr>
            <h2 id="system-stats">System Statistics</h2>

<p>Azkaban server maintains certain system statistics and they be seen http:&lt;host&gt;:&lt;port&gt;/stats</p>

<p>To enable this feature, add the following property "executor.metric.reports=true" to azkaban.properties</p>
<p>Property "executor.metric.milisecinterval.default" controls the interval at which the metrics are collected at</p>

<h3>Statistic Types</h3>
<table class="table table-striped table-bordered table-condensed">
  <thead>
    <tr>
      <th class='parameter'>Metric Name</th>
      <th class='description'>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>NumFailedFlowMetric</td>
      <td>Number of failed flows</td>
    </tr>
    <tr>
      <td>NumRunningFlowMetric</td>
      <td>Number of flows in the queue</td>
    </tr>
    <tr>
      <td>NumQueuedFlowMetric</td>
      <td>Number of flows in the queue</td>
    </tr>
    <tr>
      <td>NumRunningJobMetric</td>
      <td>Number of running jobs</td>
    </tr>
    <tr>
      <td>NumFailedJobMetric</td>
      <td>Number of failed jobs</td>
    </tr>
  </tbody>
</table>


<p>To change the statistic collection at run time, the following options are available</p>
<ul>
<li>To change the time interval at which the specific type of statistics are collected - /stats?action=changeMetricInterval&metricName=NumRunningJobMetric&interval=60000
<li>To change the duration at which the statistics are maintained -/stats?action=changeCleaningInterval&interval=604800000
<li>To change the number of data points to display - /stats?action=changeEmitterPoints&numInstances=50
<li>To enable the statistic collection - /stats?action=enableMetrics
<li>To disable the statistic collection - /stats?action=disableMetrics
</ul>

            <hr>
            <h2 id="reload-jobtypes">Reload Jobtypes</h2>

<p>When you want to make changes to your jobtype configurations or add/remove jobtypes, you can do so without restarting the executor server. You can reload all jobtype plugins as follows:</p>

<pre class="code">
curl http://localhost:EXEC_SERVER_PORT/executor?action=reloadJobTypePlugins
</pre>

          </div>

				</div>
			
			</div>
		</div>

        <div class="page-footer">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-nav">
              <li><a href="/">Documentation</a></li>
              <li><a href="http://azkaban.github.io/downloads.html">Downloads</a></li>
              <li><a href="https://github.com/azkaban">GitHub</a></li>
              <li><a href="https://groups.google.com/forum/?fromgroups#!forum/azkaban-dev">Google Group</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>



		<script>
			$(function(){
				setTimeout(function () {
					var $sideBar = $('.bs-sidebar')
					$sideBar.affix({
						offset: {
							top: function () {
								var navOuterHeight = $('.navbar').height();
								var headerHeight = $('.az-page-header').height();

								return (this.top = navOuterHeight + headerHeight);
							},
							bottom: function () {
								return (this.bottom = $('.page-footer').outerHeight(true));
							}
						}
					});
				}, 100)
			});
		</script>

	</body>
</html>

